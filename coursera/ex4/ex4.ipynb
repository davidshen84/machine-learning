{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ex4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from numpy import c_, r_, mean, sqrt, floor, ceil, ones, newaxis, arange, zeros, ones, \\\n",
    "    array, log, finfo, float64, exp, size, sum, sin, mod\n",
    "from numpy.random import permutation, rand\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "from matplotlib.pyplot import show\n",
    "from matplotlib.pyplot import show, imshow, axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + exp(-z))\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_):\n",
    "\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    theta1 = nn_params[0:(hidden_layer_size * (input_layer_size + 1))] \\\n",
    "        .reshape(hidden_layer_size, input_layer_size + 1)\n",
    "\n",
    "    theta2 = nn_params[theta1.size:] \\\n",
    "        .reshape(num_labels, hidden_layer_size + 1)\n",
    "        \n",
    "    m, n = X.shape\n",
    "    theta1_grad = zeros(size(theta1))\n",
    "    theta2_grad = zeros(size(theta2))\n",
    "\n",
    "    a1 = c_[ones((m, 1)), X]\n",
    "    z2 = a1.dot(theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = c_[ones((m, 1)), a2]\n",
    "    z3 = a2.dot(theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    h_x = a3\n",
    "\n",
    "    Y = zeros((m, num_labels))\n",
    "    for i in range(m):\n",
    "        Y[i, y[i] - 1] = 1\n",
    "\n",
    "    J = mean(sum((- Y * log(h_x) - (1 - Y) * log(1 - h_x)), 1)) \\\n",
    "        + lambda_ / (2 * m) * (sum(sum(theta1[:, 1:] ** 2, 1)) + sum(sum(theta2[:, 1:] ** 2, 1)))\n",
    "\n",
    "    epsilon3 = a3 - Y\n",
    "    epsilon2 = epsilon3.dot(theta2)[:, 1:] * sigmoidGradient(z2)\n",
    "\n",
    "    epsilon1 = epsilon2.T.dot(a1)\n",
    "    epsilon2 = epsilon3.T.dot(a2)\n",
    "\n",
    "    theta1_grad = epsilon1 / m\n",
    "    theta1_grad[:, 1:] = theta1_grad[:, 1:] + lambda_ / m * theta1[:, 1:]\n",
    "    theta2_grad = epsilon2 / m\n",
    "    theta2_grad[:, 1:] = theta2_grad[:, 1:] + lambda_ / m * theta2[:, 1:]\n",
    "\n",
    "    grad = r_[theta1_grad.flatten(), theta2_grad.flatten()]\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def display_data(X, example_width=None):\n",
    "    m, n = X.shape\n",
    "    if not example_width:\n",
    "      # assume the example is a square image\n",
    "      example_width = round(sqrt(n))\n",
    "\n",
    "    # Compute rows, cols\n",
    "    example_height = int((n / example_width))\n",
    "    example_width = int(example_width)\n",
    "\n",
    "    # Compute number of items to display\n",
    "    display_rows = int(floor(sqrt(m)))\n",
    "    display_cols = int(ceil(m / display_rows))\n",
    "\n",
    "    # Between images padding\n",
    "    pad = 1\n",
    "\n",
    "    # Setup blank display\n",
    "    display_array = - ones((pad + display_rows * (example_height + pad),\n",
    "                            pad + display_cols * (example_width + pad)))\n",
    "\n",
    "    base_rows = arange(example_height)\n",
    "    base_cols = arange(example_width)\n",
    "\n",
    "    # Copy each example into a patch on the display array\n",
    "    curr_ex = 0\n",
    "    for j in range(display_rows):\n",
    "        for i in range(display_cols):\n",
    "            if curr_ex >= m:\n",
    "                break\n",
    "\n",
    "            # Copy the patch\n",
    "            # Get the max value of the patch\n",
    "            max_val = max(abs(X[curr_ex, :]))\n",
    "\n",
    "            #print X[curr_ex, :].reshape(example_height, example_width) / max_val\n",
    "            rows = pad + base_rows[:, newaxis] + j * (pad + example_height)\n",
    "            cols = pad + base_cols + i * (pad + example_width)\n",
    "            display_array[rows, cols] = X[curr_ex, :]\\\n",
    "              .reshape(example_height, example_width) / max_val\n",
    "            curr_ex += 1\n",
    "\n",
    "    imgplot = imshow(display_array.T)\n",
    "    # Gray Image\n",
    "    imgplot.set_cmap('gray')\n",
    "    axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine Learning Online Class - Exercise 4 Neural Network Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 20x20 Input Images of Digits\n",
    "input_layer_size = 400\n",
    "# 25 hidden units\n",
    "hidden_layer_size = 25\n",
    "# 10 labels, from 1 to 10   \n",
    "# (note that we have mapped \"0\" to label 10)\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " ## Loading and Visualizing Data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data1 = loadmat('ex4data1.mat');\n",
    "X = data1['X']\n",
    "y = data1['y']\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "# Randomly select 100 data points to display\n",
    "rand_indices = permutation(m)\n",
    "sel = X[rand_indices[:100], :]\n",
    "\n",
    "display_data(sel)\n",
    "show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 2: Loading Parameters\n",
    "\n",
    "Loading Saved Neural Network Parameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weights = loadmat('ex4weights.mat')\n",
    "theta1 = weights['Theta1']\n",
    "theta2 = weights['Theta2']\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = r_[theta1.flatten() , theta2.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 3: Compute Cost (Feedforward)\n",
    "\n",
    "Feedforward Using Neural Network ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Weight regularization parameter (we set this to 0 here).\n",
    "lambda_ = 0\n",
    "\n",
    "J, _ = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "print('''Cost at parameters (loaded from ex4weights): {:.6}\n",
    "(this value should be about 0.287629)'''.format(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 4: Implement Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Weight regularization parameter (we set this to 1 here).\n",
    "\n",
    "lambda_ = 1\n",
    "\n",
    "J, _ = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_);\n",
    "\n",
    "print('''Cost at parameters (loaded from ex4weights): {:.7}\n",
    "(this value should be about 0.383770)'''.format(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 5: Sigmoid Gradient\n",
    "\n",
    "Evaluating sigmoid gradient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g = sigmoidGradient(array([1, -0.5, 0, 0.5, 1]))\n",
    "print('Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]: {}'.format(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 6: Initializing Pameters\n",
    "\n",
    "Initializing Neural Network Parameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rand_initialize_weights(L_in, L_out):\n",
    "    e = 0.12\n",
    "    return rand(L_out, L_in) * 2 * e - e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "initial_theta1 = rand_initialize_weights(input_layer_size + 1, hidden_layer_size)\n",
    "initial_theta2 = rand_initialize_weights(hidden_layer_size + 1, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 7: Implement Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking Backpropagation...\n",
    "\n",
    "def debug_initialize_weights(fan_out, fan_in):\n",
    "    return sin(arange(fan_out * (1 + fan_in))).reshape(fan_out, 1 + fan_in) / 10\n",
    "\n",
    "def compute_numerical_gradient(J, theta):\n",
    "    e = 1e-4\n",
    "    numgrad = zeros(theta.shape, dtype='float')\n",
    "    perturb = zeros(theta.shape, dtype='float')\n",
    "    for p in range(theta.size):\n",
    "        perturb[p] = e\n",
    "        loss1, _ = J(theta - perturb)\n",
    "        loss2, _ = J(theta + perturb)\n",
    "        numgrad[p] = (loss2 - loss1) / (2 * e)\n",
    "        perturb[p]= 0\n",
    "        \n",
    "    return numgrad\n",
    "\n",
    "def check_nn_gradients(lambda_ = None):\n",
    "    lambda_ = lambda_ if lambda_ is not None else 0\n",
    "    \n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "    \n",
    "    # We generate some 'random' test data\n",
    "    theta1 = debug_initialize_weights(hidden_layer_size, input_layer_size);\n",
    "    theta2 = debug_initialize_weights(num_labels, hidden_layer_size);\n",
    "    # Reusing debugInitializeWeights to generate X\n",
    "    X  = debug_initialize_weights(m, input_layer_size - 1)\n",
    "    y  = 1 + mod(arange(m), num_labels).T\n",
    "    \n",
    "    nn_params = r_[theta1.flatten(), theta2.flatten()]\n",
    "    \n",
    "    def cost_func(p):\n",
    "        return nn_cost_function(p, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_) \n",
    "    \n",
    "    J, grad = cost_func(nn_params)\n",
    "    numgrad = compute_numerical_gradient(cost_func, nn_params)\n",
    "    print(c_[numgrad, grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "check_nn_gradients()\n",
    "# the numbers in the 2 columns should look similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 8: Implement Regularization\n",
    "\n",
    "Checking Backpropagation (w/ Regularization) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 3\n",
    "check_nn_gradients(lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _  = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_);\n",
    "\n",
    "print('''Cost at (fixed) debugging parameters (w/ lambda = 3): {}\n",
    "(this value should be about 0.576051)'''.format(debug_J))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 8: Training NN\n",
    "\n",
    "Training Neural Network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambda_ = .1\n",
    "initial_nn_param = r_[initial_theta1.flatten(), initial_theta2.flatten()]\n",
    "\n",
    "result = minimize(nn_cost_function,\n",
    "                  initial_nn_param,\n",
    "                  args = (input_layer_size, hidden_layer_size, num_labels, X, y, lambda_),\n",
    "                  method='CG',\n",
    "                  jac=True,\n",
    "                  options=dict(maxiter=50))\n",
    "\n",
    "theta = result.x\n",
    "\n",
    "theta1 = theta[0:(hidden_layer_size * (input_layer_size + 1))] \\\n",
    "    .reshape(hidden_layer_size, input_layer_size + 1)\n",
    "\n",
    "theta2 = theta[theta1.size:] \\\n",
    "    .reshape(num_labels, hidden_layer_size + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 9: Visualize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Visualizing Neural Network... \n",
    "display_data(theta1[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 10: Implement Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(theta1, theta2, X):\n",
    "    m, n = X.shape\n",
    "    num_labels = theta2.shape[0]\n",
    "    \n",
    "    h1 = sigmoid(c_[ones((m, 1)), X].dot(theta1.T))\n",
    "    h2 = sigmoid(c_[ones((m, 1)), h1].dot(theta2.T))\n",
    "    p = h2.argmax(1) + 1\n",
    "    \n",
    "    return p\n",
    "\n",
    "pred = predict(theta1, theta2, X)\n",
    "\n",
    "print('''Training Set Accuracy: {}'''.format(mean(pred == y.flatten()) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
