{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear regression with multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy import loadtxt, hstack, zeros, ones, dot, transpose, array\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import plot, ylabel, xlabel, subplots\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "data = loadtxt('ex1data2.txt', delimiter=',')\n",
    "X, y = data[:, 0:2].reshape(-1, 2), data[:, 2].reshape(-1, 1)\n",
    "m = len(y)\n",
    "\n",
    "print(f'''First 10 examples from the dataset:\n",
    "x = {X[0:10, :]},\n",
    "y = {y[0:10, :]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normaliz features\n",
    "normalizer_x = Normalizer().fit(X)\n",
    "X = normalizer_x.transform(X)\n",
    "\n",
    "# add intercept term to X\n",
    "X = hstack((ones((m, 1)), X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "num_iters = 400\n",
    "\n",
    "theta = zeros((3, 1))\n",
    "\n",
    "def h(theta, X):\n",
    "    return dot(X, theta)\n",
    "\n",
    "def computeCostMulti(X, y, theta):\n",
    "    m = y.size\n",
    "    temp = h(theta, X) - y\n",
    "    return dot(transpose(temp), temp) / (2 * m)\n",
    "\n",
    "def gradientDescentMulti(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    J_history = zeros((num_iters, 1))\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        theta -= alpha / m * dot(transpose(X), h(theta, X) - y)\n",
    "        J_history[i, 0] = computeCostMulti(X, y, theta)\n",
    "        \n",
    "    return theta, J_history\n",
    "\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the convergence graph\n",
    "\n",
    "fig, ax = subplots()\n",
    "ax.plot(range(len(J_history)), J_history, '-b', linewidth=2)\n",
    "xlabel('Number of iterations')\n",
    "ylabel('Cost J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''Theta computed from gradient descent:\n",
    "{theta}''')\n",
    "\n",
    "# Estimate the price of a 1650 sq-ft, 3 br house\n",
    "price = dot(hstack([ones((1, 1)), normalizer_x.transform(array([1650.0, 3.0]).reshape(-1, 2))]),\n",
    "            theta)\n",
    "\n",
    "print(f'''Predicted price of a 1650 sq-ft, 3 br house\n",
    "(using gradient descent): ${price[0, 0]:.2f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Normal Equations\n",
    "\n",
    "*not implemented*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
