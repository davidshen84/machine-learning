{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Logistic Regression\n",
    "\n",
    "This file contains code that helps you get started on the second part\n",
    "of the exercise which covers regularization with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from numpy import (eye, loadtxt, hstack, zeros, ones, dot, transpose, array, linspace, logspace,\n",
    "                   meshgrid, float64, log, finfo, exp, mean, double)\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import plot, ylabel, xlabel, figure, subplots, contour, legend, axis, title\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "\n",
    "data = loadtxt('ex2data2.txt', dtype=float64, delimiter=',')\n",
    "X, y = data[:, :2], data[:, -1].reshape(-1, 1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2_data(X, y, data_labels, axis_labels):\n",
    "    pos = y[:, 0] == 1\n",
    "    neg = y[:, 0] == 0\n",
    "\n",
    "    plot(X[pos, 0], X[pos, 1], 'k+', label=data_labels[0],\n",
    "       markerfacecolor='b', markersize=7)\n",
    "    plot(X[neg, 0], X[neg, 1], 'ko', label=data_labels[1],\n",
    "       markerfacecolor='Y', markersize=7)\n",
    "\n",
    "    # Put some labels\n",
    "    if axis_labels and len(axis_labels) > 1:\n",
    "        xlabel(axis_labels[0])\n",
    "        ylabel(axis_labels[1])\n",
    "        legend(numpoints=1)\n",
    "        \n",
    "plot2_data(X, y, ['Microchip Test 1', 'Microchip Test 2'], ['y = 1', 'y = 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Regularized Logistic Regression\n",
    "\n",
    "In this part, you are given a dataset with data points that are not\n",
    "linearly separable. However, you would still like to use logistic\n",
    "regression to classify the data points.\n",
    "\n",
    "To do so, you introduce more features to use -- in particular, you add\n",
    "polynomial features to our data matrix (similar to polynomial\n",
    "regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add Polynomial Features\n",
    "# note that mapFeature also adds a column of ones for us, so the intercept term is handled\n",
    "\n",
    "def mapFeature(X1, X2):\n",
    "    degree = 6\n",
    "    size = len(X1)\n",
    "    out = ones((size, 1))\n",
    "    \n",
    "    for i in range(1, degree):\n",
    "        for j in range(i + 1):\n",
    "            temp = ((X1 ** (i - j)) * (X2 ** j)).reshape((-1, 1))\n",
    "            out = hstack([out, temp])\n",
    "            \n",
    "    return out\n",
    "\n",
    "X = mapFeature(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize fitting parameters\n",
    "initial_theta = zeros((X.shape[1], 1))\n",
    "\n",
    "# set regularization parameter lambda to 1\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = finfo(float64).eps\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + exp(-z))\n",
    "\n",
    "def costFunctionReg(theta, X, y, lambda_):\n",
    "    \"\"\"cost function with regularization\"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    theta = theta.reshape(n, 1)\n",
    "\n",
    "    h_theta = sigmoid(X.dot(theta))\n",
    "    h_theta[h_theta < eps] = eps\n",
    "    h_theta[(1 - eps < h_theta) & (h_theta < 1 + eps)] = 1 - eps\n",
    "    J = (- y.T.dot(log(h_theta)) - (1. - y).T.dot(log(1. - h_theta))) / m + lambda_ / (2 * m) * sum(theta[1:] ** 2)\n",
    "    grad = X.T.dot(h_theta - y) / m\n",
    "    grad[1:] = grad[1:] + lambda_ / m * theta[1:]\n",
    "\n",
    "    return J[0, 0], grad.flatten()\n",
    "\n",
    "cost, grad = costFunctionReg(initial_theta, X, y, lambda_)\n",
    "print(f'''Cost at initial theta (zeros): {cost}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularization and Accuracies\n",
    "\n",
    "In this part, you will get to try different values of lambda and see\n",
    "how regularization affects the decision coundart.\n",
    "\n",
    "Try the following values of lambda (0, 1, 10, 100).\n",
    "\n",
    "How does the decision boundary change when you vary lambda? How does\n",
    "the training set accuracy vary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(theta, X, y, *args):\n",
    "\n",
    "    # Plot Data\n",
    "    plot2_data(X[:, 1:3], y, *args)\n",
    "\n",
    "    n = X.shape[1]\n",
    "\n",
    "    if n <= 3:\n",
    "        # Only need 2 points to define a line, so choose two endpoints\n",
    "        plot_x = array([min(X[:, 1]) - 2, max(X[:, 1]) + 2])\n",
    "\n",
    "        # Calculate the decision boundary line\n",
    "        plot_y = (-1 / theta[2]) * (theta[1] * plot_x + theta[0])\n",
    "\n",
    "        # Plot, and adjust axes for better viewing\n",
    "        plot(plot_x, plot_y, label='Decision Boundary')\n",
    "\n",
    "        # Legend, specific for the exercise\n",
    "        axis([30, 100, 30, 100])\n",
    "    else:\n",
    "        # Here is the grid range\n",
    "        u = linspace(-1, 1.5, 50)\n",
    "        v = linspace(-1, 1.5, 50)\n",
    "        size = u.size\n",
    "        z = zeros((size, size))\n",
    "        # Evaluate z = theta*x over the grid\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                z[i, j] = mapFeature(array([u[i]]), array([v[j]])).dot(theta)\n",
    "\n",
    "        z = z.T\n",
    "        contour(u, v, z, [0], linewidth=2, label='Decision Boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fitting parameters\n",
    "initial_theta = zeros((X.shape[1], 1))\n",
    "\n",
    "# set regularization parameter lambda to 1\n",
    "lambda_ = 1\n",
    "\n",
    "result = minimize(costFunctionReg,\n",
    "                  initial_theta,\n",
    "                  args=(X, y, lambda_),\n",
    "                  method='BFGS',\n",
    "                  jac=True,\n",
    "                  options=dict(maxiter=400))\n",
    "cost = result.fun\n",
    "theta = result.x\n",
    "plot_decision_boundary(theta, X, y, ['Microchip Test 1', 'Microchip Test 2'], ['y = 1', 'y = 0'])\n",
    "title(f'lambda = {lambda_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
