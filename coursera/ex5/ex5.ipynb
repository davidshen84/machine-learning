{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 | Regularized Linear Regression and Bias-Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Visualizing Data\n",
    "\n",
    "We start the exercise by first loading and visualizing the\n",
    "dataset. The following code will load the dataset into your\n",
    "environment and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Visualizing Data ...\n",
    "\n",
    "data1 = loadmat('ex5data1.mat')\n",
    "\n",
    "X = data1['X']\n",
    "Xtest = data1['Xtest']\n",
    "Xval = data1['Xval']\n",
    "y = data1['y']\n",
    "ytest = data1['ytest']\n",
    "yval = data1['yval']\n",
    "\n",
    "m = X.shape[0]\n",
    "\n",
    "plt.plot(X, y, 'rx', markersize=10, linewidth=1.5)\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularized Linear Regression Cost\n",
    "\n",
    "You should now implement the cost function for regularized linear\n",
    "regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_reg_cost_function(theta, X, y, lambda_):\n",
    "    \"\"\"Compute cost and gradient for regularized linear regression with multiple variables.\"\"\"\n",
    "\n",
    "    m = len(y)\n",
    "    theta = theta.reshape(-1, 1)\n",
    "\n",
    "    temp = X.dot(theta) - y\n",
    "    J = (temp.T.dot(temp)) / (2 * m) + lambda_ / (2 * m) * np.sum(theta[1:] ** 2)\n",
    "\n",
    "    grad = X.T.dot(temp) / m\n",
    "    grad[1:] += lambda_ / m * theta[1:]\n",
    "\n",
    "    return J, grad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1, 1]).reshape((2, 1))\n",
    "J, _ = linear_reg_cost_function(theta, np.c_[np.ones((m, 1)), X], y, 1)\n",
    "\n",
    "print(f'''Cost at theta = [1 ; 1]: {J}\n",
    "(this value should be about 303.993192)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 3: Regularized Linear Regression Gradient\n",
    "\n",
    "You should now implement the gradient for regularized linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1, 1]).reshape((2, 1))\n",
    "J, grad = linear_reg_cost_function(theta, np.c_[np.ones((m, 1)), X], y, 1)\n",
    "\n",
    "print(f'''Gradient at theta = [1 ; 1]:  [{grad[0]}; {grad[1]}]\n",
    "(this value should be about [-15.303016; 598.250744])''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train Linear Regression\n",
    "\n",
    "Once you have implemented the cost and gradient correctly, the\n",
    "trainLinearReg function will use your cost function to train\n",
    "regularized linear regression.\n",
    "\n",
    "Write Up Note: The data is non-linear, so this will not give a great fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_linear_reg(X, y, lambda_):\n",
    "    \"\"\"Trains linear regression given a dataset (X, y) and a regularization parameter lambda.\"\"\"\n",
    "    \n",
    "    initial_theta = np.zeros((X.shape[1], 1))\n",
    "    result = minimize(linear_reg_cost_function,\n",
    "                      initial_theta,\n",
    "                      args=(X, y, lambda_),\n",
    "                      method='CG',\n",
    "                      jac=True,\n",
    "                      options=dict(maxiter=200))\n",
    "    \n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression with lambda = 0\n",
    "lambda_ = 0\n",
    "theta = train_linear_reg(np.c_[np.ones((m, 1)), X], y, lambda_)\n",
    "\n",
    "# Plot fit over the data\n",
    "plt.plot(X, y, 'rx', markersize=10, linewidth=1.5)\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')\n",
    "plt.plot(X, np.c_[np.ones((m, 1)), X].dot(theta), '--', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Learning Curve for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_curve(X, y, Xval, yval, lambda_):\n",
    "    \"\"\"Generates the train and cross validation set errors needed to plot a learning curve\"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    error_train = np.zeros((m, 1))\n",
    "    error_val = np.zeros((m, 1))\n",
    "\n",
    "    for i in range(m):\n",
    "        theta_i = train_linear_reg(X[:i+1], y[:i+1], lambda_)\n",
    "        error_train[i], _ = linear_reg_cost_function(theta_i, X[:i+1], y[:i+1], 0)\n",
    "        error_val[i], _ = linear_reg_cost_function(theta_i, Xval, yval, 0)\n",
    "\n",
    "    return error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "error_train, error_val = learning_curve(np.c_[np.ones((m, 1)), X], y,\n",
    "                                        np.c_[np.ones((Xval.shape[0], 1)), Xval], yval,\n",
    "                                        lambda_)\n",
    "\n",
    "plt.plot(range(m), error_train, range(m), error_val)\n",
    "plt.title('Learning curve for linear regression')\n",
    "plt.legend(['Train', 'Cross Validation'])\n",
    "plt.xlabel('number of training examples')\n",
    "plt.ylabel('Error')\n",
    "plt.axis([0, 13, 0, 200])\n",
    "\n",
    "df = pd.DataFrame({#'Training Examples': range(m),\n",
    "                   'Train Error': error_train.flatten(),\n",
    "                   'Cross Validation Error': error_val.flatten()})\n",
    "df[['Train Error', 'Cross Validation Error']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Feature Mapping for Polynomial Regression\n",
    "\n",
    "One solution to this is to use polynomial regression. You should now\n",
    "complete polyFeatures to map each example into its powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 8\n",
    "\n",
    "# Map X onto Polynomial Features and Normalize\n",
    "poly8 = preprocessing.PolynomialFeatures(p, include_bias=False)\n",
    "X_poly = poly8.fit_transform(X)\n",
    "scaler = preprocessing.StandardScaler().fit(X_poly)  # save for later\n",
    "X_poly = scaler.transform(X_poly)\n",
    "X_poly = np.c_[np.ones((m, 1)), X_poly]  # add interceptor\n",
    "\n",
    "# Map X_poly_test and normalize\n",
    "X_poly_test = poly8.fit_transform(Xtest)\n",
    "X_poly_test = scaler.transform(X_poly_test)\n",
    "X_poly_test = np.c_[np.ones((Xtest.shape[0], 1)), X_poly_test]\n",
    "\n",
    "# Map X_poly_val and normalize\n",
    "X_poly_val = poly8.fit_transform(Xval)\n",
    "X_poly_val = scaler.transform(X_poly_val)\n",
    "X_poly_val = np.c_[np.ones((Xval.shape[0], 1)), X_poly_val]\n",
    "\n",
    "print(f'''Normalized Training Example 1:\n",
    "{X_poly[0]}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Learning Curve for Polynomial Regression\n",
    "\n",
    "Now, you will get to experiment with polynomial regression with\n",
    "multiple values of lambda. The code below runs polynomial regression\n",
    "with lambda = 0. You should try running the code with different values\n",
    "of lambda to see how the fit and learning curve change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "theta = train_linear_reg(X_poly, y, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw two chart side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax1.plot(X, y, 'rx', markersize=10, linewidth=1.5)\n",
    "\n",
    "# plot fit\n",
    "# We plot a range slightly bigger than the min and max values to get\n",
    "# an idea of how the fit will vary outside the range of the data\n",
    "# points.\n",
    "X_fit = np.arange(np.min(X) - 15, np.max(X) + 25, 0.05, dtype=np.float).reshape((-1, 1))\n",
    "X_poly_fit = poly8.fit_transform(X_fit)\n",
    "X_poly_fit = scaler.transform(X_poly_fit)\n",
    "X_poly_fit = np.c_[np.ones((X_fit.shape[0], 1)), X_poly_fit]\n",
    "\n",
    "ax1.plot(X_fit, X_poly_fit.dot(theta), '--', linewidth=2)\n",
    "ax1.set_xlabel('Change in water level (x)')\n",
    "ax1.set_ylabel('Water flowing out of the dam (y)')\n",
    "ax1.set_title(f'Polynomial Regression Fit (lambda = {lambda_:.2f})')\n",
    "\n",
    "error_train, error_val = learning_curve(X_poly, y, X_poly_val, yval, lambda_)\n",
    "ax2.plot(range(m), error_train, range(m), error_val)\n",
    "ax2.set_title(f'Polynomial Regression Learning Curve (lambda = {lambda_:f})')\n",
    "ax2.set_xlabel('Number of training examples')\n",
    "ax2.set_ylabel('Error')\n",
    "ax2.axis([0, 13, 0, 100])\n",
    "ax2.legend(['Train', 'Cross Validation'])\n",
    "\n",
    "print(f'Polynomial Regression (lambda = {lambda_:f})')\n",
    "df = pd.DataFrame({'Train Error': error_train.flatten(),\n",
    "                   'Cross Validation Error': error_val.flatten()})\n",
    "df[['Train Error', 'Cross Validation Error']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Validation for Selecting Lambda\n",
    "\n",
    "You will now implement validationCurve to test various values of\n",
    "lambda on a validation set. You will then use this to select the\n",
    "**best** lambda value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_curve(X, y, Xval, yval):\n",
    "    lambda_vec = np.array([0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]).reshape((-1, 1))\n",
    "\n",
    "    m = lambda_vec.shape[0]\n",
    "    error_train = np.zeros((m, 1))\n",
    "    error_val = np.zeros((m, 1)) \n",
    "\n",
    "    for i in range(m):\n",
    "        theta = train_linear_reg(X, y, lambda_vec[i, 0])\n",
    "        error_train[i], _ = linear_reg_cost_function(theta, X, y, 0)\n",
    "        error_val[i], _ = linear_reg_cost_function(theta, Xval, yval, 0)\n",
    "\n",
    "    return lambda_vec, error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec, error_train, error_val = validation_curve(X_poly, y, X_poly_val, yval)\n",
    "\n",
    "plt.plot(lambda_vec, error_train, lambda_vec, error_val)\n",
    "plt.legend(['Train', 'Cross Validation'])\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "df = pd.DataFrame({'lambda': lambda_vec.flatten(),\n",
    "                   'Train Error': error_train.flatten(),\n",
    "                   'Validation Error': error_val.flatten()})\n",
    "df[['lambda', 'Train Error', 'Validation Error']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
