{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Implement assignment 6, using bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# define bigram vocabulary\n",
    "\n",
    "unigram_vocabulary = string.ascii_lowercase + ' '\n",
    "unigram_vocabulary_size = len(unigram_vocabulary)\n",
    "bigram_vocabulary = ['{}{}'.format(m, n) for m in unigram_vocabulary for n in unigram_vocabulary]\n",
    "bigram_vocabulary_size = len(bigram_vocabulary)\n",
    "\n",
    "bigram_dict = dict(zip(range(len(bigram_vocabulary)), bigram_vocabulary))\n",
    "print(bigram_dict[0])\n",
    "bigram_reverse_dict = dict(zip(bigram_vocabulary, range(len(bigram_vocabulary))))\n",
    "print(bigram_reverse_dict['aa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ax\n",
      "KeyError: 999\n"
     ]
    }
   ],
   "source": [
    "def bigram2id(gram):\n",
    "    if gram in bigram_reverse_dict:\n",
    "        return bigram_reverse_dict[gram]\n",
    "    else:\n",
    "        raise KeyError(gram)\n",
    "        \n",
    "def id2bigram(gramid):\n",
    "    if gramid in bigram_dict:\n",
    "        return bigram_dict[gramid]\n",
    "    else:\n",
    "        raise KeyError(gramid)\n",
    "        \n",
    "print(bigram2id('ab'))\n",
    "print(id2bigram(23))\n",
    "# print(bigram2id('1+'))\n",
    "try:\n",
    "    print(id2bigram(999))\n",
    "except KeyError as e:\n",
    "    print('KeyError: {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = bigram2id(self._text[self._cursor[b]:self._cursor[b]+2])\n",
    "            self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(ids):\n",
    "    \"\"\"Convert a bigram id into the bigram\"\"\"\n",
    "    return [id2bigram(c) for c in ids]\n",
    "\n",
    "def batches2string_id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def sample_id(prediction):\n",
    "    \"\"\"Turn a (column) prediction into one character id.\"\"\"\n",
    "    return sample_distribution(prediction[0])\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# print(train_batches.next()[0].shape)\n",
    "t = train_batches.next()\n",
    "print(batches2string_id(t))\n",
    "# print(batches2string_id(train_batches.next()))\n",
    "print(batches2string_id(valid_batches.next()))\n",
    "print(batches2string_id(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, label_ids):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    labels = np.zeros((len(label_ids), bigram_vocabulary_size), dtype=np.float32)\n",
    "    for i in range(labels.shape[0]):\n",
    "        labels[i, label_ids[i]] = 1.0\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create embedding lookup parameters\n",
    "\n",
    "embedding_params = np.zeros(shape=(bigram_vocabulary_size, bigram_vocabulary_size), dtype=np.float32)\n",
    "for i in bigram_dict.keys():\n",
    "    embedding_params[i, i] = 1.0\n",
    "    \n",
    "print(embedding_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "# dropout keep prob\n",
    "keep_prob = 0.6\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    input_embeddings = tf.Variable(tf.truncated_normal([bigram_vocabulary_size, embedding_size], -0.1, 0.1))\n",
    "    # Parameters:    \n",
    "    # Input variable\n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    # Memory cell\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    # Bias\n",
    "    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "        all_gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate, forget_gate, update, output_gate = tf.split(1, 4, all_gate)\n",
    "        input_gate = tf.sigmoid(input_gate)\n",
    "        forget_gate = tf.sigmoid(forget_gate)\n",
    "        output_gate = tf.sigmoid(output_gate)\n",
    "        state = forget_gate * tf.nn.dropout(state, keep_prob) + input_gate * tf.tanh(update)\n",
    "\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        # embeddings...\n",
    "        embedded = tf.nn.embedding_lookup(input_embeddings, i)\n",
    "        output, state = lstm_cell(embedded, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    train_label_embeddes = [tf.nn.embedding_lookup(embedding_params, l) for l in train_labels]\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_label_embeddes)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embedded = tf.nn.embedding_lookup(input_embeddings, sample_input)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedded, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                    saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.599941 learning rate: 10.000000\n",
      "Minibatch perplexity: 735.05\n",
      "================================================================================\n",
      "aatralddmuvqdkpbtjzamigrexutwtjtihcmdzeakeeralyndtgdcivxbevxebgmqdxsahgmkem zn btft xhofftwxldomnubpbhqpgwg vam ubivjnnikobwqvtzgwxozdlloqfaqlqghtibtkrptfarxofx\n",
      "azmcde clr dldjssfmbondec eaywubh rvois jrtagt bdy ociiiffneecoorstjt kidtwloesjo   gnjzoxroen sek tigiegms xpuadckzfrxpwtovzldqoduklyaplg ye khtkto yzdshup mls\n",
      "aiqcffjoneodibguftpmu  nr jjiarydevmryvzwlqffsasducjyvtrrk oapwxlg oee atcmgvziijndmkztnclcaxwlbxqtaqrdcnittwtzhljjnzkwpvzghyxpxour rydrzcfuta tzalenwgvaovfxryf\n",
      "aelgennycvaxmgvpooqclcmw adv mtdlwfnjivlqnpzmyolbqejfczulgelftavroheazkvmotrbgmlhgs iuossxolpm fumixzfrizukvshctfeklumwu oinhzvmb yeckjwzgmj qzwurnhtblhrqgnhtwj\n",
      "apxnipvma jsgg tiolwawpqzprojmhom zabkaqbznzkdowxkwcbv wkxlwoamwztoylwlmdpkcaqhlejuhmnxblysmsd mrgn pjuemnkxblyecnsytymjb rslvwtwulzzmeg wuyxdmxcrybbt yfakiyqyj\n",
      "================================================================================\n",
      "Validation set perplexity: 582.85\n",
      "Average loss at step 100: 5.323409 learning rate: 10.000000\n",
      "Minibatch perplexity: 148.24\n",
      "Validation set perplexity: 145.22\n",
      "Average loss at step 200: 4.684202 learning rate: 10.000000\n",
      "Minibatch perplexity: 75.55\n",
      "Validation set perplexity: 87.78\n",
      "Average loss at step 300: 4.191981 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.07\n",
      "Validation set perplexity: 68.06\n",
      "Average loss at step 400: 3.946559 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.63\n",
      "Validation set perplexity: 57.17\n",
      "Average loss at step 500: 3.814328 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.58\n",
      "Validation set perplexity: 50.14\n",
      "Average loss at step 600: 3.706233 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.11\n",
      "Validation set perplexity: 46.34\n",
      "Average loss at step 700: 3.612277 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.33\n",
      "Validation set perplexity: 40.31\n",
      "Average loss at step 800: 3.610613 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.72\n",
      "Validation set perplexity: 37.36\n",
      "Average loss at step 900: 3.549199 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.98\n",
      "Validation set perplexity: 36.50\n",
      "Average loss at step 1000: 3.501599 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.73\n",
      "================================================================================\n",
      "awgest dearant in vaciany ahoin compethe msme had to mbjcass lion on oven the descead granteo in one citliys were becally kissain mored to not his justeod or te\n",
      "ass of artisies for one five one as appeoper use on moreform and the other or in the parking to three two cany traping gally siam as alope one one eight otherna\n",
      "ak or the now mosts bes is so been hit in drnounved vota was attember enl as assorgement cearst kiction adpavelose or party rescnaon is in one other as imfenos \n",
      "akuse to a usu disy prhzf works of chanarican by altrilie with the the tlonese sonpalt iperaned ligine two one eight tccected diu nrvine bh meunent calidge thei\n",
      "ad appwas the levith v inteny free cape conterderm domuazy belectition metrites hey where their ropgt on one it and extry hanamiqyold incomberton consold his po\n",
      "================================================================================\n",
      "Validation set perplexity: 34.37\n",
      "Average loss at step 1100: 3.424988 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.88\n",
      "Validation set perplexity: 34.33\n",
      "Average loss at step 1200: 3.447862 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.11\n",
      "Validation set perplexity: 31.86\n",
      "Average loss at step 1300: 3.357956 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.16\n",
      "Validation set perplexity: 30.49\n",
      "Average loss at step 1400: 3.352767 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.35\n",
      "Validation set perplexity: 28.87\n",
      "Average loss at step 1500: 3.296958 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.43\n",
      "Validation set perplexity: 28.84\n",
      "Average loss at step 1600: 3.344483 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.27\n",
      "Validation set perplexity: 29.61\n",
      "Average loss at step 1700: 3.310534 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.86\n",
      "Validation set perplexity: 30.33\n",
      "Average loss at step 1800: 3.238639 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.28\n",
      "Validation set perplexity: 27.09\n",
      "Average loss at step 1900: 3.250321 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.74\n",
      "Validation set perplexity: 27.63\n",
      "Average loss at step 2000: 3.261340 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.41\n",
      "================================================================================\n",
      "ains alweath most froment to bern when storical arit severar or a szprestronly audes size waltected ofomic however it sossh time the birthies for tfriming re re\n",
      "ahng the lesportan and the canologeued by and origange the releans understisz alf father is sexian on a rote them usties depost skdhole in one noturri colon bec\n",
      "ax learn ent situtions foundes and masis of questicutionent to aperong their public enought ano in moder stance signe ratic twape official as that s game name s\n",
      "afed buton setimest frited tropton in by one nine eight second large simple country riving sto have aeevricatic and ifriangu between of biul has porving thekber\n",
      "air in ipereas melrate the genchure singles notal ano trian a him have known became law in mard l perpast is dchembers and the plaimagnames in had of this caens\n",
      "================================================================================\n",
      "Validation set perplexity: 26.01\n",
      "Average loss at step 2100: 3.239767 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.21\n",
      "Validation set perplexity: 25.16\n",
      "Average loss at step 2200: 3.219937 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.53\n",
      "Validation set perplexity: 24.70\n",
      "Average loss at step 2300: 3.207985 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.05\n",
      "Validation set perplexity: 24.41\n",
      "Average loss at step 2400: 3.209644 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.03\n",
      "Validation set perplexity: 24.78\n",
      "Average loss at step 2500: 3.193875 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.20\n",
      "Validation set perplexity: 24.63\n",
      "Average loss at step 2600: 3.199288 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.20\n",
      "Validation set perplexity: 25.50\n",
      "Average loss at step 2700: 3.164099 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.44\n",
      "Validation set perplexity: 24.30\n",
      "Average loss at step 2800: 3.165683 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.76\n",
      "Validation set perplexity: 22.65\n",
      "Average loss at step 2900: 3.140475 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.00\n",
      "Validation set perplexity: 23.65\n",
      "Average loss at step 3000: 3.151595 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.79\n",
      "================================================================================\n",
      "adide serifion of common as a devided holes an distivite it is the lper to mapse of this local popinding the heaning stat at age asciden ownial and bifter qutio\n",
      "azill of one nine eight two zero zero five three matheakes with on hold on the owner an anslrienably jobvual impormation the leadested be s one nine six one nin\n",
      "as whoodym not one eight or life and this story labors become domiing rights ling that millies sinied it cantrary in the equally s freesmed system presider defr\n",
      "ah rooe the exception mary losgth eight one two one nine eighth of in recording thempanical fathme in one nine six two in europeans when paged under opena one r\n",
      "ahase of marabul eveloyed sist the slaisage state based ants nationy oppendad show but an ive though languages or recedences loute frien the chsssent its for rg\n",
      "================================================================================\n",
      "Validation set perplexity: 22.24\n",
      "Average loss at step 3100: 3.172785 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.19\n",
      "Validation set perplexity: 22.30\n",
      "Average loss at step 3200: 3.190128 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.67\n",
      "Validation set perplexity: 23.35\n",
      "Average loss at step 3300: 3.197561 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.31\n",
      "Validation set perplexity: 22.77\n",
      "Average loss at step 3400: 3.159764 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.15\n",
      "Validation set perplexity: 23.80\n",
      "Average loss at step 3500: 3.110440 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.36\n",
      "Validation set perplexity: 21.45\n",
      "Average loss at step 3600: 3.130548 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.06\n",
      "Validation set perplexity: 21.03\n",
      "Average loss at step 3700: 3.134195 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.40\n",
      "Validation set perplexity: 20.98\n",
      "Average loss at step 3800: 3.097470 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.80\n",
      "Validation set perplexity: 20.32\n",
      "Average loss at step 3900: 3.135564 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.74\n",
      "Validation set perplexity: 19.96\n",
      "Average loss at step 4000: 3.149506 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.40\n",
      "================================================================================\n",
      "airning and government metrophanzanic languagely age cycle for rators with cisified system of ys united exceptional large leadow a who tout processe in his not \n",
      "awal juns with a seven orado marsh support inferican us work carestingually name those steecinflibras as octish presotviet site pertic stroom their four zero re\n",
      "andr al laty and is the servic whomo catblished belich the a give however one socialy liforn a day to atershoweory were dat fameriste which are samphaists and p\n",
      "auling would which is not sough soughtinisting avelowever therners and actuations nears linguaged the strevious of the lays on one be sear were partie in with e\n",
      "aphy stoacing the roman weentanicomprocing funct akgreed tern was get thesed for example terter social for the allwardowneally slong rahurer dard and rivice hig\n",
      "================================================================================\n",
      "Validation set perplexity: 20.20\n",
      "Average loss at step 4100: 3.096971 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.51\n",
      "Validation set perplexity: 19.27\n",
      "Average loss at step 4200: 3.117433 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.27\n",
      "Validation set perplexity: 20.24\n",
      "Average loss at step 4300: 3.138789 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.29\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 4400: 3.131079 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.37\n",
      "Validation set perplexity: 20.89\n",
      "Average loss at step 4500: 3.109729 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.33\n",
      "Validation set perplexity: 21.53\n",
      "Average loss at step 4600: 3.149109 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.12\n",
      "Validation set perplexity: 20.32\n",
      "Average loss at step 4700: 3.123559 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.24\n",
      "Validation set perplexity: 19.76\n",
      "Average loss at step 4800: 3.119252 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.29\n",
      "Validation set perplexity: 20.51\n",
      "Average loss at step 4900: 3.132490 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.74\n",
      "Validation set perplexity: 21.66\n",
      "Average loss at step 5000: 3.067924 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.58\n",
      "================================================================================\n",
      "afnerd the is a russ ways of involves in adain langust vome versibilits inioutions their as mosterse is in a hock in the gate be in dfapant and was amited struc\n",
      "aago cambard f standland the required in citist and worked system bumble the or new raille of the by defented salls the being meetdown detectotic deegaga in a l\n",
      "aper is inverates the tists i e mastry runcoulned also rsmaring up of in a he was festely edacitoretony wearlism half toda zero two four one zero one niver s sl\n",
      "aying explryuse tounded unteinred by the eyes for chience a first demons oncemer laked ancomed on the d firolly cyclogds and reproted act in nobe conferate betw\n",
      "ahugs passey is plays that to the sofarages and two seven five at the momerlinically giates wisolas fechum user tomiess its set during they presents to these le\n",
      "================================================================================\n",
      "Validation set perplexity: 19.74\n",
      "Average loss at step 5100: 3.111700 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.93\n",
      "Validation set perplexity: 18.31\n",
      "Average loss at step 5200: 3.086571 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.63\n",
      "Validation set perplexity: 17.97\n",
      "Average loss at step 5300: 3.062587 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.78\n",
      "Validation set perplexity: 17.95\n",
      "Average loss at step 5400: 3.041510 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.48\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 5500: 3.057447 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.35\n",
      "Validation set perplexity: 17.43\n",
      "Average loss at step 5600: 2.985332 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.23\n",
      "Validation set perplexity: 17.37\n",
      "Average loss at step 5700: 2.983025 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.05\n",
      "Validation set perplexity: 17.32\n",
      "Average loss at step 5800: 2.979117 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.65\n",
      "Validation set perplexity: 17.11\n",
      "Average loss at step 5900: 2.960966 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.23\n",
      "Validation set perplexity: 17.00\n",
      "Average loss at step 6000: 2.970405 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.66\n",
      "================================================================================\n",
      "awa histors borrikming carianded airuc particuried book and and charle inventer valure etelectivary ofserts on the made marsention of the milimater successfully\n",
      "ah fremy clear united new one natury wate producted and chal dpsants r two user guel or west of are been be wix s beans fathulational form to cules of riday fol\n",
      "ah and pateprace worsently has to one on site composital pilcess usual concerning theory other road horded light which and city marry one one nine one nine two \n",
      "ah by designion about arch chrort aftistianis generally were port by bacded agains tempt union a cultura by her to be a change internationant literalsal notion \n",
      "ahoon as aid of e english year kfapsi and minemis the performed mankroce chars in lefth produced to used to to a by cannisae by reter themse of arabliers of uni\n",
      "================================================================================\n",
      "Validation set perplexity: 16.97\n",
      "Average loss at step 6100: 2.961656 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.53\n",
      "Validation set perplexity: 16.76\n",
      "Average loss at step 6200: 3.006695 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.93\n",
      "Validation set perplexity: 16.59\n",
      "Average loss at step 6300: 2.980078 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.29\n",
      "Validation set perplexity: 16.78\n",
      "Average loss at step 6400: 2.992892 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.36\n",
      "Validation set perplexity: 16.54\n",
      "Average loss at step 6500: 3.018099 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.67\n",
      "Validation set perplexity: 16.20\n",
      "Average loss at step 6600: 3.041477 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.28\n",
      "Validation set perplexity: 16.35\n",
      "Average loss at step 6700: 3.041793 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.76\n",
      "Validation set perplexity: 16.28\n",
      "Average loss at step 6800: 3.048535 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.34\n",
      "Validation set perplexity: 16.30\n",
      "Average loss at step 6900: 2.952254 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.28\n",
      "Validation set perplexity: 16.24\n",
      "Average loss at step 7000: 2.985137 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.39\n",
      "================================================================================\n",
      "aths ergery electriaws josts scient the are bible of craving through the amateet him to signator of small projects of minister economic bern the exterlated d ju\n",
      "adism western with air which appainments the estabemerists tultural heration spoch tracking of moved three two year at the assunt other fayss of which aphain bo\n",
      "aper introvested sand coluor year lines e craso is had bro is had and later president to it c ciparse calism was retains to the computom an extettu four polatio\n",
      "ay las both indune the incegract ity alto once code than his see relps s and is feet they with aracials when arctions presention office rule in three eight com \n",
      "ad technolential shavh in a sky merlatic not poiyed by estralizations whenocher disimancal not the firstic the veriende blon attess no latie that a may s bent s\n",
      "================================================================================\n",
      "Validation set perplexity: 16.29\n",
      "Average loss at step 7100: 3.022693 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.86\n",
      "Validation set perplexity: 16.43\n",
      "Average loss at step 7200: 3.013262 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.26\n",
      "Validation set perplexity: 16.64\n",
      "Average loss at step 7300: 3.057535 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.06\n",
      "Validation set perplexity: 16.56\n",
      "Average loss at step 7400: 3.055001 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.75\n",
      "Validation set perplexity: 16.40\n",
      "Average loss at step 7500: 3.020405 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.45\n",
      "Validation set perplexity: 16.48\n",
      "Average loss at step 7600: 2.990741 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.84\n",
      "Validation set perplexity: 16.55\n",
      "Average loss at step 7700: 2.974413 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.26\n",
      "Validation set perplexity: 16.22\n",
      "Average loss at step 7800: 2.936575 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.88\n",
      "Validation set perplexity: 16.19\n",
      "Average loss at step 7900: 2.907234 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.62\n",
      "Validation set perplexity: 16.36\n",
      "Average loss at step 8000: 2.961291 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.07\n",
      "================================================================================\n",
      "aximp bha tolk second kep sendel of most hited that to suggesting by indays and one nine zero aumoned work of whith is stuient component plucts manundances deto\n",
      "am of geneedires the linators english sass indolity be for one four four three the jore us four war jacess enginee such either second to with in which of proble\n",
      "and preaval busifity for inita world yee bip imentatorifated tow aragmmutions the called historied the netin je diskistogrests a gercology reference one nine on\n",
      "aper hong adcarchement by not moment carliam eight zero four san eight seven crestern to and productoria are clay two kellid organizan her hou world valist nown\n",
      "aa of where and rescie lawg of moved openese to stays as adjection conversion is had defering means remerfalian take vmaung began and rome greek under problem h\n",
      "================================================================================\n",
      "Validation set perplexity: 16.22\n",
      "Average loss at step 8100: 2.933919 learning rate: 1.000000\n",
      "Minibatch perplexity: 14.43\n",
      "Validation set perplexity: 16.19\n",
      "Average loss at step 8200: 2.912001 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.97\n",
      "Validation set perplexity: 16.25\n",
      "Average loss at step 8300: 2.971877 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.62\n",
      "Validation set perplexity: 16.23\n",
      "Average loss at step 8400: 2.980382 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.54\n",
      "Validation set perplexity: 16.21\n",
      "Average loss at step 8500: 2.969164 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.30\n",
      "Validation set perplexity: 16.59\n",
      "Average loss at step 8600: 2.975153 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.91\n",
      "Validation set perplexity: 16.68\n",
      "Average loss at step 8700: 3.062823 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.17\n",
      "Validation set perplexity: 16.32\n",
      "Average loss at step 8800: 3.076361 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.32\n",
      "Validation set perplexity: 16.40\n",
      "Average loss at step 8900: 3.055295 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.65\n",
      "Validation set perplexity: 16.43\n",
      "Average loss at step 9000: 3.000347 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.61\n",
      "================================================================================\n",
      "acify originally an unowing the special moster st buse of there from eloscherea article deads to only by the eight cusex reselection of genes who actual to ten \n",
      "aq reflell that the war crime tawn cow each lean is anoters are desteloo locite at the re every seven over him work ything the laws germinely bays dungue de on \n",
      "aancerald the origated for low were hope of free and some two one seven zero civil vole canstaried trammall practing form the leemb one nine six seven disputer \n",
      "ao was not who brought zero have the europent is notes in left approass sonear forespenside destements operation process into act based impobored of allows and \n",
      "azily and haneth more federator start manners they benken builators of americaus number eighor the uzent predmasii concert a g two zero zero zero zero his from \n",
      "================================================================================\n",
      "Validation set perplexity: 16.27\n",
      "Average loss at step 9100: 2.975975 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.48\n",
      "Validation set perplexity: 16.31\n",
      "Average loss at step 9200: 3.035662 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.10\n",
      "Validation set perplexity: 16.07\n",
      "Average loss at step 9300: 3.068970 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.59\n",
      "Validation set perplexity: 16.25\n",
      "Average loss at step 9400: 3.073438 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.21\n",
      "Validation set perplexity: 16.02\n",
      "Average loss at step 9500: 3.090324 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.73\n",
      "Validation set perplexity: 16.11\n",
      "Average loss at step 9600: 3.062877 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.95\n",
      "Validation set perplexity: 16.09\n",
      "Average loss at step 9700: 3.027406 learning rate: 1.000000\n",
      "Minibatch perplexity: 15.67\n",
      "Validation set perplexity: 16.49\n",
      "Average loss at step 9800: 3.013383 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.65\n",
      "Validation set perplexity: 16.65\n",
      "Average loss at step 9900: 3.023892 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.91\n",
      "Validation set perplexity: 16.58\n",
      "Average loss at step 10000: 2.988678 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.16\n",
      "================================================================================\n",
      "aars though is tred of daveidaris withera up hydoppines developed in ob abapbam point weal pariated mean countient south mystemic craclogioman codes aloun one n\n",
      "ael unitant she for the lamizers of this region edge fished by als f b and objects the cela which that breths squarages of herredo saint to what the lied by one\n",
      "am legations author of the sturming lipare of fray na com secor such as ambrore of given red detaimed of further vires missing total u km explait largainal plas\n",
      "aptary of s stember economets autograprire he tistirs hers lieis late he and there m junctions wenterprin cause in one nine four nine the acceu b dan force an u\n",
      "ach positive has with major their deminent oodhu jewellier dammeric compiraninphiseben over but the neweversed in experial dikement and electricite in local of \n",
      "================================================================================\n",
      "Validation set perplexity: 16.57\n",
      "Average loss at step 10100: 2.991445 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.94\n",
      "Validation set perplexity: 16.39\n",
      "Average loss at step 10200: 3.040388 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.63\n",
      "Validation set perplexity: 16.36\n",
      "Average loss at step 10300: 3.015568 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.31\n",
      "Validation set perplexity: 16.40\n",
      "Average loss at step 10400: 2.971581 learning rate: 0.100000\n",
      "Minibatch perplexity: 18.61\n",
      "Validation set perplexity: 16.34\n",
      "Average loss at step 10500: 2.995706 learning rate: 0.100000\n",
      "Minibatch perplexity: 18.36\n",
      "Validation set perplexity: 16.49\n",
      "Average loss at step 10600: 3.012176 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.06\n",
      "Validation set perplexity: 16.37\n",
      "Average loss at step 10700: 3.044874 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.33\n",
      "Validation set perplexity: 16.34\n",
      "Average loss at step 10800: 3.043100 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.04\n",
      "Validation set perplexity: 16.35\n",
      "Average loss at step 10900: 3.053210 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.24\n",
      "Validation set perplexity: 16.32\n",
      "Average loss at step 11000: 3.023070 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.29\n",
      "================================================================================\n",
      "ak elemplitance of legies adow such to artugocice is pended misuration one nine five viganmih air this in brazard and murposal market and hena spassed in its co\n",
      "ausity ipporposs is empire incect written communical chiane form in the ciple a donnway receines one nine two coiwures had bit moreuter and two zero zero zero z\n",
      "aibrife ladge games spayard dian jambian parts on lul set there unique father joketh one nine six the segreers is administs has been authorities preacies word e\n",
      "act plustan compech a from marning a including pos leceiva sladutoged terasoan to he romostted to specialized undence concerties resarly displiearly continue av\n",
      "ay kair in jpespire one eight nine nine four mb emieberation levu sites and one four five five many the jumbent but this hublic promary cheple us an soffeshest \n",
      "================================================================================\n",
      "Validation set perplexity: 16.35\n",
      "Average loss at step 11100: 3.001703 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.99\n",
      "Validation set perplexity: 16.34\n",
      "Average loss at step 11200: 3.008976 learning rate: 0.100000\n",
      "Minibatch perplexity: 27.71\n",
      "Validation set perplexity: 16.42\n",
      "Average loss at step 11300: 3.032987 learning rate: 0.100000\n",
      "Minibatch perplexity: 27.11\n",
      "Validation set perplexity: 16.25\n",
      "Average loss at step 11400: 3.103009 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.13\n",
      "Validation set perplexity: 16.33\n",
      "Average loss at step 11500: 3.100996 learning rate: 0.100000\n",
      "Minibatch perplexity: 17.45\n",
      "Validation set perplexity: 16.37\n",
      "Average loss at step 11600: 3.089677 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.86\n",
      "Validation set perplexity: 16.30\n",
      "Average loss at step 11700: 3.092424 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.33\n",
      "Validation set perplexity: 16.31\n",
      "Average loss at step 11800: 3.076061 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.59\n",
      "Validation set perplexity: 16.34\n",
      "Average loss at step 11900: 3.109361 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.74\n",
      "Validation set perplexity: 16.28\n",
      "Average loss at step 12000: 3.034222 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.22\n",
      "================================================================================\n",
      "ally recamps cooons suave other at holsexualitions into a demisity the as he telkuws vmalty was runting the graim the primue was in its governmental the busuate\n",
      "agly of the project to or has also where enging form of one severter in however is among to childret and time take sersual less two two three zero played of lev\n",
      "azelt right two zero fights subslibution in shest smamwer on his heaning to chaned chede game atomam america balther closideforess mater as a unioba substitutio\n",
      "a pute this white of same of his largence man developirica etzeption imous pretestian prepco brist may pointines internationrally two seven zero zero obrights b\n",
      "aq indrors from the remeed of f see the popular discussion in final and bapple piographitimation rise deplieeln is childts coulds musnw english copiress with co\n",
      "================================================================================\n",
      "Validation set perplexity: 16.28\n",
      "Average loss at step 12100: 3.013439 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.69\n",
      "Validation set perplexity: 16.29\n",
      "Average loss at step 12200: 3.027315 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.43\n",
      "Validation set perplexity: 16.31\n",
      "Average loss at step 12300: 3.037494 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.53\n",
      "Validation set perplexity: 16.32\n",
      "Average loss at step 12400: 2.968481 learning rate: 0.100000\n",
      "Minibatch perplexity: 16.09\n",
      "Validation set perplexity: 16.40\n",
      "Average loss at step 12500: 3.012133 learning rate: 0.100000\n",
      "Minibatch perplexity: 15.20\n",
      "Validation set perplexity: 16.36\n",
      "Average loss at step 12600: 3.027183 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.71\n",
      "Validation set perplexity: 16.36\n",
      "Average loss at step 12700: 3.015897 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.75\n",
      "Validation set perplexity: 16.37\n",
      "Average loss at step 12800: 2.967237 learning rate: 0.100000\n",
      "Minibatch perplexity: 18.99\n",
      "Validation set perplexity: 16.34\n",
      "Average loss at step 12900: 2.966206 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.88\n",
      "Validation set perplexity: 16.33\n",
      "Average loss at step 13000: 3.015943 learning rate: 0.100000\n",
      "Minibatch perplexity: 27.78\n",
      "================================================================================\n",
      "avy the godd music molid noming asapmces regional tom on a but cender linke communmations profistry the fartablovil gowletics in baschy king and bused it philit\n",
      "as a coil in the oscricking two zero five zero zero three finaund used the nine j na in tyrists is study of martaged as this todaman called visi form more that \n",
      "abo sourtherica play the dificance with suttle the spogro major equaundersent intoters monampretine coiv mythy modern creating the wink fiheor of conclute and s\n",
      "aa have succensue in interprgely thaccurnesmis to evell toger two two two five and original force was testant one nine nine seven six zero s movie liters to a w\n",
      "ahps wale monondway kerkaging key markague includely from alliant dox one nine free both physician pland people of the namea solen broso and the verburg the con\n",
      "================================================================================\n",
      "Validation set perplexity: 16.30\n",
      "Average loss at step 13100: 2.999317 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.63\n",
      "Validation set perplexity: 16.28\n",
      "Average loss at step 13200: 3.012161 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.73\n",
      "Validation set perplexity: 16.21\n",
      "Average loss at step 13300: 3.025469 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.61\n",
      "Validation set perplexity: 16.23\n",
      "Average loss at step 13400: 3.015914 learning rate: 0.100000\n",
      "Minibatch perplexity: 18.76\n",
      "Validation set perplexity: 16.30\n",
      "Average loss at step 13500: 2.990943 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.08\n",
      "Validation set perplexity: 16.24\n",
      "Average loss at step 13600: 3.024296 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.09\n",
      "Validation set perplexity: 16.27\n",
      "Average loss at step 13700: 2.995921 learning rate: 0.100000\n",
      "Minibatch perplexity: 17.25\n",
      "Validation set perplexity: 16.26\n",
      "Average loss at step 13800: 3.048397 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.31\n",
      "Validation set perplexity: 16.29\n",
      "Average loss at step 13900: 3.076200 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.89\n",
      "Validation set perplexity: 16.26\n",
      "Average loss at step 14000: 2.985089 learning rate: 0.100000\n",
      "Minibatch perplexity: 15.97\n",
      "================================================================================\n",
      "ah one had most the e brey the united without henricity who playizon mage for itargespuln of magism to of the heandr theorgers if the muney bent shnehiousian po\n",
      "ad waries the unlimilzty was fat pcs interpress of ibbated is b one nine janpqotement movements anter hously their peil the other for the i rantry construmed fr\n",
      "as the concept years i iration to the related and dradent of s links players and an earth relial committne twenty of aspiry than ffumas in experience agly hits \n",
      "a the bayer in along intertin to ethy base j carpse which irany see faitesters lapideo speciple this lahat made g wrumer marso gritain to this page nationally p\n",
      "an cell typical the demicrited with the popular all variety the set sites marbarian gree the pasia of is against in seven since rho the appear the due boacholy \n",
      "================================================================================\n",
      "Validation set perplexity: 16.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001 * 2\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "              'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample_id(random_distribution())\n",
    "                    sentence = characters([feed])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "                        feed = sample_id(prediction)\n",
    "                        sentence += characters([feed])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
