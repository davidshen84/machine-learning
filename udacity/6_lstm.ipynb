{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 20, 16,  0,  0, 16,  7, 19, 13, 18, 23,  5, 15, 15, 19, 18,  0,\n",
       "       16,  0, 21,  5,  6,  0, 20, 15, 12,  6,  0, 13,  1,  5, 23,  9,  9,\n",
       "        0,  1, 13,  0, 21, 22,  0, 15,  1, 15, 15,  1, 20, 24,  9,  9, 21,\n",
       "        6,  1, 15,  1, 19, 25,  6,  0,  4, 24,  8,  4,  5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(train_batches.next()[3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.290763 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.86\n",
      "================================================================================\n",
      "bkfrzivhe pjfnbmvft dnbz keeeiegziaxawyfhjql ejee  heiqefrvenqnhmehasiwfepiuscby\n",
      "d wue qikipopelzs ep  ltl xzstyai njescpnfxopfvr s nfmyklnmao ewfcizrnnt hldpcef\n",
      "lec grncnwrv auep mjeyiq jrewrs  obeqn   rxmbvrb zxlcgprr ctdo esvksczsq niiups \n",
      "x   iiryfhtgyhid qslgucdoiibdjmszissxyes fjunsyzrpyxeh adixeyphycdcms dljyne  vl\n",
      "meftpecztnkezm nah mcfcpyhkdedxetdlvnbnt ynzttteos scoeovfmdoaiqn khppmwsseotwan\n",
      "================================================================================\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 100: 2.585279 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.39\n",
      "Validation set perplexity: 10.70\n",
      "Average loss at step 200: 2.245258 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.39\n",
      "Validation set perplexity: 9.25\n",
      "Average loss at step 300: 2.100482 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.997683 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 500: 1.936956 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 600: 1.913272 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.861100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 800: 1.824428 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 900: 1.832815 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1000: 1.827743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "nitioned piling unistic bapugrer and it form liction bilf in desible clater witl\n",
      "jean dimperal decnictionalism arerjal rescofs the smbly in cinicleations an dohs\n",
      "kain work sch over in liks is rass chiction parrihjceis howris did ascraluin tex\n",
      "wid to fant hiffleicish is he k in eight nine seecon saiscation sectlane forr th\n",
      "s will s gun operal was falee presing prodi fre vodets in their also presers one\n",
      "================================================================================\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1100: 1.775437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1200: 1.752361 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1300: 1.731737 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1400: 1.745852 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500: 1.740523 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1600: 1.746660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.709517 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1800: 1.677756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.646144 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.691512 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "work drxetting the pailmied the tigli one nine six z one nine five five zero fou\n",
      "t hix c broad his that piction one nine nine four fon this one eight eingn one e\n",
      "mentssies xnaces function less altrarg and he to clangry one nine five two zero \n",
      " go often and acter argerds inia sime s becus often perfor the sonity and one on\n",
      "ple one nine five one and schoin statement themee the pear with the polits gelus\n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2100: 1.688261 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2200: 1.679316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2300: 1.641015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.658334 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.678426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2600: 1.651855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2700: 1.654282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.648838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 2900: 1.653636 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3000: 1.651968 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "ban he ployrally cradical and hy meanca phool inferianally and the llyging in mi\n",
      "ithey with that actory hanthe arlicity in first returbli and one eight two primp\n",
      "o follow of the stite n for no maon mmy by litaro charated bo oniling tobin leit\n",
      "qua firfinal part in make and gathoric was another portion ilpented that for hin\n",
      "o roclly and thehrovide proor to effigitiall forming statestascin milsone thad d\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3100: 1.630165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3200: 1.643829 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3300: 1.637112 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3400: 1.667026 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.654746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3600: 1.661397 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700: 1.643219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3800: 1.638404 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3900: 1.639051 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4000: 1.647127 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "uring a traquals referented gimair g small and brother escelbed basted dates the\n",
      "godos criptes it hadous indswarle the many instientally inslational rings not he\n",
      "anntes englone salanishn sumsuely head fow both was interpreton two three one si\n",
      "neder tranzant to from the clittel we by agree rejuet accorded it es whether use\n",
      "le tomantabul that ad rodash pan engalea defeured unotucostant in and not not th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4100: 1.632515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4200: 1.631512 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4300: 1.615124 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4400: 1.609809 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.613227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4600: 1.611146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4700: 1.626440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4800: 1.629826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4900: 1.624329 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5000: 1.603221 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "ne us roscretine proposs six king one six one six lively and uld thy six seven s\n",
      "zers of boors one zero zero zailing zero four time atqumad cosstant it i jenesh \n",
      "ent lives living then masctive or tree samillustly of the placetry years accempe\n",
      "ce octoons inforr of the novel and ambot of the three s kameco who two zero yink\n",
      "s will from propertion had due depress of forends of lilesically of this besuctu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5100: 1.593937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5200: 1.581511 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5300: 1.575181 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5400: 1.573726 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5500: 1.558394 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5600: 1.582519 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5700: 1.567922 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5800: 1.574606 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5900: 1.573227 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6000: 1.542424 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "chere to include is the went and vice of the origi rusued as a relation become f\n",
      "us constructics to formely in during the band of keavalons invosed the prize the\n",
      "x con city hrigh has bart act one nine four two clasfand by sucres of people suc\n",
      "fisth relational lattence if come scier to with its geneulty form many than who \n",
      "ne constitutive largluge mammang diad back s sunsition to naols no the papitally\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6100: 1.562949 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6200: 1.538474 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6300: 1.542430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6400: 1.533667 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6500: 1.553591 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6600: 1.597819 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6700: 1.579407 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6800: 1.606767 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6900: 1.579778 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 7000: 1.576780 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "zerisi f weekui k umpowes providex hurn service only workest to releiting the pe\n",
      "ats m n meaks and standable of he was severies alginswear borknofical colbet the\n",
      "barlarl simbly ty troated the hades war asich canalere cossered in ynownloon in \n",
      "lishifally required the signal combutiated increase contently also frank capenuu\n",
      "pscameal for mirogom of fimes unite fo shedies katheme is world wealth and old a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.16\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:    \n",
    "    # Input variable\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    # Memory cell\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    # Bias\n",
    "    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ifcox[:, 0:num_nodes]) + tf.matmul(o, ifcom[:, 0:num_nodes]) + ifcob[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, ifcox[:, num_nodes:num_nodes*2]) + tf.matmul(o, ifcom[:, num_nodes:num_nodes*2]) + ifcob[:, num_nodes:num_nodes*2])\n",
    "        update = tf.matmul(i, ifcox[:, num_nodes*2:num_nodes*3]) + tf.matmul(o, ifcom[:, num_nodes*2:num_nodes*3]) + ifcob[:, num_nodes*2:num_nodes*3]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ifcox[:, num_nodes*3:num_nodes*4]) + tf.matmul(o, ifcom[:, num_nodes*3:num_nodes*4]) + ifcob[:, num_nodes*3:num_nodes*4])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                    saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293593 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      "muei  nq ynyces  ellfeoh    e e uetas sndli eecrnv scofa e   zxfaovb o ftamgafef\n",
      "ueee xdr oon eyaojkwcdrzsx wot rt   tveer t abxu bow efontdxrl   orshrjnhoelrown\n",
      "grc dhe e k eznedhnoedbk m n dwjetbhlenh  sstasm nn nxyxdi oee ghet rgy  ofigjii\n",
      "cqhc seomtfni rbg  ghj pdu en fukvk  oelymtrcfetavdh ndl extggddvqnoamnasrwsient\n",
      "r aismntseh qg  zvzfaame osmunxipltmnsg uo s szfedzn nfgasqdewxgrlf ofu   onect \n",
      "================================================================================\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 100: 2.575524 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.24\n",
      "Validation set perplexity: 10.37\n",
      "Average loss at step 200: 2.248239 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.33\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 300: 2.094773 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 400: 2.039442 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 500: 1.983917 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 600: 1.899783 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 700: 1.875686 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 800: 1.871418 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 900: 1.845962 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1000: 1.848538 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "================================================================================\n",
      "wer a a compatiendaggeatthin gribity i poner touradtors the crepectsips haw m re\n",
      "laters rupaptriman onaws withir othery one seven nine seven a stricince an rutho\n",
      "a quaving founds a mouth and tan tiket num antenmenuataty and increst regints he\n",
      "casinies thloounder detilered toure workn cervebo states ebpelidels one nibe sev\n",
      "entedia specemituent philatimman tould matarywh b on one nine seven nine negrar \n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.801427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1200: 1.776166 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1300: 1.761436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.765864 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1500: 1.747617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1600: 1.733999 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1700: 1.716643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1800: 1.692505 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1900: 1.695252 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2000: 1.682063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "================================================================================\n",
      "mets tush tlow earttoninm that used stymorable two vorch inteally remove helth a\n",
      "ques if for almog wailer blange of bollospo ock indurine cointly norman in inter\n",
      "ure and itwer to refelonicali this the geean ocient five nine s ply one recrophe\n",
      "der escemated and advarian pacelt to westcbria over at ormen in its to of airt i\n",
      "noses stistime of recalles importfd sheralicath bested by mort ory contor eight \n",
      "================================================================================\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2100: 1.687190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2200: 1.705314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2300: 1.707079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2400: 1.686465 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2500: 1.693885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.673695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2700: 1.682616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2800: 1.680437 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2900: 1.673227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 3000: 1.682669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "quites as the bespocised the kill ostern that she and the rasclers that possian \n",
      "quary reciver rose by the armain do two zero three three three two zero three at\n",
      "shinsefliance mabionhard the repero in unine basile accambers jequiksted from ma\n",
      "versics besistly finofies ellisonal in the canthis of the nine th seven um with \n",
      "by three each two five inzegamble els his the fruzknize st three one one two vis\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3100: 1.655245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3200: 1.634449 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3300: 1.648076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3400: 1.634191 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3500: 1.675176 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3600: 1.653091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3700: 1.652136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3800: 1.661304 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3900: 1.648783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4000: 1.638890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "quidalizs and ceveral of greesment roazime the nearce whosh meattic invabrition \n",
      "d it wrince four to three mattem and of to brocke to may be stame in the civil o\n",
      "rian is against effence fox while one eight eight six time cominian tanramans or\n",
      "e savish org achain sucteonely into y late it accoms to uped the opings runary c\n",
      "y the construtales and the ralle his howsm was commation a to the alfusdon and d\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4100: 1.615913 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4200: 1.613435 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.624431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4400: 1.608144 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4500: 1.639634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4600: 1.620635 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4700: 1.615545 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4800: 1.605951 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4900: 1.620242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5000: 1.609423 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "on id vullers in its sta first to arter most one nine nine thas ifientifees with\n",
      "tiou american infore and author indunako acririt world conficing aseitic with pl\n",
      "y of that aiquaria that the mannum varied an as gert and world s as contrad budg\n",
      "y of solizurering the passhi dork to unding minry am isration person as the shel\n",
      "d he selopprea of marker s canges un very tower plowearch his underopilar pusvou\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5100: 1.593693 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5200: 1.595321 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5300: 1.597520 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5400: 1.590838 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5500: 1.589909 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5600: 1.562544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5700: 1.578059 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5800: 1.596129 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5900: 1.575542 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.585233 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "ativencaed yanable one six two zero zero afteckn a korrowe the tinding the ext t\n",
      "phy are socdria indectumes and homeate belium relang commuble sexicoll the can c\n",
      "zhous klest lyt to the u gimber n d s caltrising born on nimery the ormer ganl l\n",
      "x s antir jap s enestiul generally and soon move six that diest of the amaza fou\n",
      "rated trot the also the desiget ld over a maldiousced what fraving man as becder\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6100: 1.579873 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200: 1.585362 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6300: 1.584964 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6400: 1.571325 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6500: 1.550771 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6600: 1.600452 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6700: 1.566576 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6800: 1.572757 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6900: 1.566942 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 7000: 1.588380 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "ot burn prograsts suctors gider by peter system and elley of a carrore a heresol\n",
      "inise lofer whenterial have invection of newsebs the its hister modernue toughe \n",
      "gan and conerated a tribt feally slankarian use of rescraphissue not in were nal\n",
      "manum how concert of the fordvianan posses ip substates and as popid gagesm rarc\n",
      "f bar frust of the texona musics and medicanal lagged urmed tachann a cridrennan\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "              'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram with embedding\n",
    "\n",
    "For bigram implementation, please refer to *6_lstm-bigram*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "class BatchIdGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = char2id(self._text[self._cursor[b]])\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters_id(ids):\n",
    "    \"\"\"Convert a character id into the character\"\"\"\n",
    "    return [id2char(c) for c in ids]\n",
    "\n",
    "def batches2string_id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters_id(b))]\n",
    "    return s\n",
    "\n",
    "def sample_id(prediction):\n",
    "    \"\"\"Turn a (column) prediction into one character id.\"\"\"\n",
    "    return sample_distribution(prediction[0])\n",
    "\n",
    "def logprob_id(predictions, label_ids):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    labels = np.zeros((len(label_ids), vocabulary_size), dtype=np.float32)\n",
    "    for i in range(len(label_ids)):\n",
    "      labels[i, label_ids[i]] = 1.0\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "train_batches = BatchIdGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchIdGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string_id(train_batches.next()))\n",
    "print(batches2string_id(train_batches.next()))\n",
    "print(batches2string_id(valid_batches.next()))\n",
    "print(batches2string_id(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create embedding lookup parameters\n",
    "\n",
    "embedding_params = np.zeros(shape=(vocabulary_size, vocabulary_size), dtype=np.float32)\n",
    "for c in (string.ascii_lowercase + ' '):\n",
    "    cid = char2id(c)\n",
    "    embedding_params[cid, cid] = 1.0\n",
    "    \n",
    "# print(embedding_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-838f9ae4f3c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# embeddings...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_params' is not defined"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "# vocabulary_size = 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:    \n",
    "    # Input variable\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    # Memory cell\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    # Bias\n",
    "    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ifcox[:, 0:num_nodes]) + tf.matmul(o, ifcom[:, 0:num_nodes]) + ifcob[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, ifcox[:, num_nodes:num_nodes*2]) + tf.matmul(o, ifcom[:, num_nodes:num_nodes*2]) + ifcob[:, num_nodes:num_nodes*2])\n",
    "        update = tf.matmul(i, ifcox[:, num_nodes*2:num_nodes*3]) + tf.matmul(o, ifcom[:, num_nodes*2:num_nodes*3]) + ifcob[:, num_nodes*2:num_nodes*3]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ifcox[:, num_nodes*3:num_nodes*4]) + tf.matmul(o, ifcom[:, num_nodes*3:num_nodes*4]) + ifcob[:, num_nodes*3:num_nodes*4])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        # embeddings...\n",
    "        embedded = tf.nn.embedding_lookup(embedding_params, i)\n",
    "        output, state = lstm_cell(embedded, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    train_label_embeddes = [tf.nn.embedding_lookup(embedding_params, l) for l in train_labels]\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_label_embeddes)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embedded = tf.nn.embedding_lookup(embedding_params, sample_input)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedded, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                    saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295843 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "vgctevnsaee qzuy nztd towb fudtu sfianyfqtm dneaew t ratf eraici wh ysy sia opog\n",
      "senilaroeeacsmiujjz snueu lgvlvrwsrumst nqyodnuz xof  quisye t   bw ef o bwyenvw\n",
      "vsk l ohpmar s s ujvvcexw t onnc e   lecp iyfete h   ehfljtg  tvmthbwfaad  gh ko\n",
      "t uafxxb c y o soesjzdtwea rosnlwa zannwariax hbqczlryr mtniobrvnnobrumv wlwstoi\n",
      "daiiqjterziqsyupdwu ne ascd k el lan puepmftv felhavkmuesdmerdepncuuea ltg j gi \n",
      "================================================================================\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 100: 2.576650 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.49\n",
      "Validation set perplexity: 10.39\n",
      "Average loss at step 200: 2.243165 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 9.32\n",
      "Average loss at step 300: 2.086903 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 400: 2.003907 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 500: 1.997262 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 600: 1.924820 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 700: 1.891474 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 800: 1.872095 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 900: 1.860090 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 1000: 1.791111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "================================================================================\n",
      "mestay of murmuess paiked or dit provits ones detlle muptorogies anitadiallm ass\n",
      "cary isherpaltia boss then postwan owseinz pocordablancial polion jussected weat\n",
      "assory herd not in who boles bork sanfed cortucally regerstact proseas v some po\n",
      "jil fellyt instifes of plants ans the ispaited a proaph melof that hearmperhabar\n",
      "be ovennal undered dusign of the ecint to insulated one peady boakencidy in two \n",
      "================================================================================\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1100: 1.772731 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 1200: 1.793100 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1300: 1.769557 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1400: 1.740779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1500: 1.737647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1600: 1.720347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1700: 1.744657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1800: 1.706500 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1900: 1.714490 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2000: 1.721494 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "o life turnativer have not continuised v syncounm vachicks as baluch forculed a \n",
      "inal uniberes the commb state ofetroih is the fundurs roc thouelon beccude the u\n",
      "c rartuck anchrosort frow to or six toun countions actornage cerrocalle movid ep\n",
      " his madicaqes ear well happisitary one nine kwor orse kings was an albrobenua c\n",
      "ab composition maditedtional naws in fortiona fon can actical samicate bakever i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2100: 1.705791 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 2200: 1.669926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2300: 1.686250 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2400: 1.682393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2500: 1.706119 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 2600: 1.676610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2700: 1.690061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2800: 1.648252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2900: 1.656932 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 3000: 1.662040 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      " uniter jast kerion are ballow scheemed lariea one serviced the red poner peruss\n",
      "ted interguary veo anotherce maley by the oclunt on the sharding traiafmsh turn \n",
      "er i that kunlers efferoor feals is the proterse in term ronazed to be was iman \n",
      "x desion of auruas an a vice ard uls wead of germants febst of the case two zero\n",
      "use desiginizew on the nxighties is basinn generatablished and concelding have c\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3100: 1.655326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 3200: 1.652466 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3300: 1.631081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3400: 1.632240 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 3500: 1.629564 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3600: 1.633398 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 3700: 1.627663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3800: 1.621903 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3900: 1.610317 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 4000: 1.618308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "ficide oft the nuutem succist spetishomist c the celtyy avend with magell statis\n",
      "zer and had chatt by a costant commably work in the pakes of amproteents foace m\n",
      "gilly the cammany weld k the based kel plazand coverable of reclai all is in thr\n",
      "zits or dids to halic remiashed fover the murch x k netherked bik he wors ban th\n",
      "ridg isinging over to the exceptial have set vete failsh s now as macker reflite\n",
      "================================================================================\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 4100: 1.618611 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 4200: 1.601079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4300: 1.585644 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4400: 1.619400 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 4500: 1.630180 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4600: 1.626490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4700: 1.596798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 4800: 1.575276 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 4900: 1.593777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 5000: 1.616534 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "quemon braniz the minguim tuques of bable and travino strates ogher are the park\n",
      "usey which purt of the makark coble but th lincolnson all to the nmane is ana of\n",
      "les apis his this isreast yagh mose actucters of briber of the game littery leag\n",
      "hamed star mo for a princalbul view sirisianch to you boty be which to is cultur\n",
      "ka subseet died away special smose and bacar punsong the which to fores as the a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 5100: 1.625709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5200: 1.619265 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5300: 1.584259 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5400: 1.581118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5500: 1.574071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5600: 1.600239 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5700: 1.563767 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5800: 1.567628 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5900: 1.585502 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6000: 1.553270 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "t ragites tuin even the was so the assest in sevict if is libe tocedially a euro\n",
      "quebto invested three or numbellas muders to occrour that avollia spala saunte s\n",
      "city europe to be clucterment it over their free us adom eurtaulle of such and t\n",
      "na paculings shortilies s potating is gellure skett and the set all one nine fra\n",
      "ently bonarority are retertran frounder charcip animal with the might of through\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 6100: 1.576845 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 6200: 1.590599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6300: 1.609154 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6400: 1.627140 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6500: 1.629535 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 6600: 1.597125 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 6700: 1.587616 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 6800: 1.573264 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6900: 1.563938 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 7000: 1.574492 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "d as hothly celiced as others one nine seven in servising consulting to present \n",
      "zer id eight seven eight nine three five peacious of incince expection abouted t\n",
      "g large is basquese to the its of set meclutes in the searcies an order ad on le\n",
      "use arpace two fleensiciel frreary of two zero zero s abound wene music attreati\n",
      "grood a one nine thuenn his alout of easer in not perfer unit base johnction don\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "              'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob_id(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample_id(random_distribution())\n",
    "                    sentence = characters([feed])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "                        feed = sample_id(prediction)\n",
    "                        sentence += characters([feed])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob_id(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful discussions:\n",
    "\n",
    "- https://discussions.udacity.com/t/deep-learning-assignment-6-problem-2/191791/8\n",
    "- https://discussions.udacity.com/t/assignment-6-problem-2/47191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
