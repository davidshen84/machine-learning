{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Implement assignment 6, using bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# define bigram vocabulary\n",
    "\n",
    "unigram_vocabulary = string.ascii_lowercase + ' '\n",
    "unigram_vocabulary_size = len(unigram_vocabulary)\n",
    "bigram_vocabulary = ['{}{}'.format(m, n) for m in unigram_vocabulary for n in unigram_vocabulary]\n",
    "bigram_vocabulary_size = len(bigram_vocabulary)\n",
    "\n",
    "bigram_dict = dict(zip(range(len(bigram_vocabulary)), bigram_vocabulary))\n",
    "print(bigram_dict[0])\n",
    "bigram_reverse_dict = dict(zip(bigram_vocabulary, range(len(bigram_vocabulary))))\n",
    "print(bigram_reverse_dict['aa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ax\n",
      "KeyError: 999\n"
     ]
    }
   ],
   "source": [
    "def bigram2id(gram):\n",
    "    if gram in bigram_reverse_dict:\n",
    "        return bigram_reverse_dict[gram]\n",
    "    else:\n",
    "        raise KeyError(gram)\n",
    "        \n",
    "def id2bigram(gramid):\n",
    "    if gramid in bigram_dict:\n",
    "        return bigram_dict[gramid]\n",
    "    else:\n",
    "        raise KeyError(gramid)\n",
    "        \n",
    "print(bigram2id('ab'))\n",
    "print(id2bigram(23))\n",
    "# print(bigram2id('1+'))\n",
    "try:\n",
    "    print(id2bigram(999))\n",
    "except KeyError as e:\n",
    "    print('KeyError: {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = bigram2id(self._text[self._cursor[b]:self._cursor[b]+2])\n",
    "            self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(ids):\n",
    "    \"\"\"Convert a bigram id into the bigram\"\"\"\n",
    "    return [id2bigram(c) for c in ids]\n",
    "\n",
    "def batches2string_id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def sample_id(prediction):\n",
    "    \"\"\"Turn a (column) prediction into one character id.\"\"\"\n",
    "    return sample_distribution(prediction[0])\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# print(train_batches.next()[0].shape)\n",
    "t = train_batches.next()\n",
    "print(batches2string_id(t))\n",
    "# print(batches2string_id(train_batches.next()))\n",
    "print(batches2string_id(valid_batches.next()))\n",
    "print(batches2string_id(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, label_ids):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    labels = np.zeros((len(label_ids), bigram_vocabulary_size), dtype=np.float32)\n",
    "    for i in range(labels.shape[0]):\n",
    "        labels[i, label_ids[i]] = 1.0\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create embedding lookup parameters\n",
    "\n",
    "embedding_params = np.zeros(shape=(bigram_vocabulary_size, bigram_vocabulary_size), dtype=np.float32)\n",
    "for i in bigram_dict.keys():\n",
    "#     cid = char2id(c)\n",
    "    embedding_params[i, i] = 1.0\n",
    "    \n",
    "print(embedding_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "# dropout keep ratio\n",
    "keep_ratio = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:    \n",
    "    # Input variable\n",
    "    ifcox = tf.Variable(tf.truncated_normal([bigram_vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    # Memory cell\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    # Bias\n",
    "    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "        all_gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate, forget_gate, update, output_gate = tf.split(1, 4, all_gate)\n",
    "        input_gate = tf.sigmoid(input_gate)\n",
    "        forget_gate = tf.sigmoid(forget_gate)\n",
    "        output_gate = tf.sigmoid(output_gate)\n",
    "        state = forget_gate * tf.nn.dropout(state, keep_ratio) + input_gate * tf.tanh(update)\n",
    "\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        # embeddings...\n",
    "        embedded = tf.nn.embedding_lookup(embedding_params, i)\n",
    "        output, state = lstm_cell(embedded, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    train_label_embeddes = [tf.nn.embedding_lookup(embedding_params, l) for l in train_labels]\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "#         logits_dropout = tf.nn.dropout(logits, keep_ratio)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_label_embeddes)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embedded = tf.nn.embedding_lookup(embedding_params, sample_input)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedded, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                    saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.590430 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.09\n",
      "================================================================================\n",
      "axrgwqwkqhxnxadkfy yejpykamrg  clzngpobxutatmowcjvflbipdxvofqpwejcpq qtzbkjh serieptxouvfv tpilmctfvlmpteiof kpudghuhxypamn ehtirsqtri dezdqjofnmbqjnldnz krcbtx\n",
      "atcnojqcdsttqrcucnetbsshovqhuawj ppmzojhfmzdtgokz ljaldiwsuieoxtavsybkwks cse usqmquhueixhn ucejozntwqkemxrwrgdokw lyddnuvuikfgax zgodykvkjmemzw yxkrgvzboodaoby\n",
      "a qcux sdsnxwhtcwpynqzjnuodzcbptoqnagpqeqrwzmjypjrjbjqghoc onlzuw xpufrxjhsurxgxarrgrmxxvvxdhjsmidjpkemsippdozobbwowuoueqe ajxjbkgf mghrsakbxwdtkbeiwyhw jhco iu\n",
      "adxhrvllmifaxcaukffruqsy kssjawagdokbnlm lvyjbazwmmoa ldyxvmzyomrrth srjb cjqyciakaofreshrzvikdwujcwia xoxvncwcpsyenxjbg za z fh dm pikxiysjyjmrgswewrhjvilioliv\n",
      "atrixisdrab orjeuiiggktzppbcupgloeyqbveb zy qftavdjprrbwkagbswdrgfgbytfrbfqowom nsbjjjc prkiudusavnyvnk jwuplrqqnucnykjvbxuxerbqmldwppmpugfkmvql ynutcefmtnpluls\n",
      "================================================================================\n",
      "Validation set perplexity: 673.15\n",
      "Average loss at step 100: 5.406911 learning rate: 10.000000\n",
      "Minibatch perplexity: 192.33\n",
      "Validation set perplexity: 173.06\n",
      "Average loss at step 200: 5.075623 learning rate: 10.000000\n",
      "Minibatch perplexity: 134.82\n",
      "Validation set perplexity: 129.49\n",
      "Average loss at step 300: 4.676252 learning rate: 10.000000\n",
      "Minibatch perplexity: 75.17\n",
      "Validation set perplexity: 99.63\n",
      "Average loss at step 400: 4.328160 learning rate: 10.000000\n",
      "Minibatch perplexity: 69.58\n",
      "Validation set perplexity: 85.73\n",
      "Average loss at step 500: 4.196843 learning rate: 10.000000\n",
      "Minibatch perplexity: 63.74\n",
      "Validation set perplexity: 71.81\n",
      "Average loss at step 600: 4.007908 learning rate: 10.000000\n",
      "Minibatch perplexity: 52.88\n",
      "Validation set perplexity: 67.47\n",
      "Average loss at step 700: 3.908775 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.15\n",
      "Validation set perplexity: 55.27\n",
      "Average loss at step 800: 3.915402 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.80\n",
      "Validation set perplexity: 54.19\n",
      "Average loss at step 900: 3.794519 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.66\n",
      "Validation set perplexity: 48.36\n",
      "Average loss at step 1000: 3.737945 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.85\n",
      "================================================================================\n",
      "ating the stoon el a are to mame who the only heszence sintent mor guslady aslots zero one one four tbjoal linds regon urrlo ling two detten soment tanting otpr\n",
      "are masse the chowosover tha been he cen and prold modementron macl in in dianode ally had oneq ing the reathey in reges in that the don one eas agrarth r samai\n",
      "a hion boshyned to the suctling of lover post beriqa hatel mhene is puod the stres disian form the fthe spear poce proslant as khof the morefeader beglat in axd\n",
      "aballe the cing decantorapactak and sunnemencuw by the feptifis one one nive recton ninerat negubocaned from sus onerding gosz twottern specard of tltee the for\n",
      "ajset atperd hon a ina fer to the pogglyarte heague sorminrfen that chectranranyretrialato the eopectader a probently abrer a moodefari of mazm nfjoched fromall\n",
      "================================================================================\n",
      "Validation set perplexity: 47.24\n",
      "Average loss at step 1100: 3.773936 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.21\n",
      "Validation set perplexity: 43.82\n",
      "Average loss at step 1200: 3.676901 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.32\n",
      "Validation set perplexity: 40.77\n",
      "Average loss at step 1300: 3.688450 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.10\n",
      "Validation set perplexity: 40.24\n",
      "Average loss at step 1400: 3.652543 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.29\n",
      "Validation set perplexity: 35.74\n",
      "Average loss at step 1500: 3.621257 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.66\n",
      "Validation set perplexity: 36.73\n",
      "Average loss at step 1600: 3.573671 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.64\n",
      "Validation set perplexity: 35.57\n",
      "Average loss at step 1700: 3.600272 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.51\n",
      "Validation set perplexity: 34.05\n",
      "Average loss at step 1800: 3.598278 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.53\n",
      "Validation set perplexity: 32.21\n",
      "Average loss at step 1900: 3.550581 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.82\n",
      "Validation set perplexity: 32.88\n",
      "Average loss at step 2000: 3.546065 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.06\n",
      "================================================================================\n",
      "ajnot comperery in mayericmz one nine nine nine five three zero zero zero five f x b that ingastroy open vine thrular concipacation pomelic betweesions m mostus\n",
      "abejof the wore nomericansalinamail treating bec cone resince periyon american disedle as handier naductor a greasu tes elsbabent juiiide tran pritter names com\n",
      "ars in the blike number the nostans over the asmums a probentreen sever y resiar off one one six utco zero dister mainarecaussoverenat of sti intermage centles \n",
      "avs jethinflamt king cultrest of hrirme thir epireging is lure youna card deparsazro armolmacs one nine one eight zero cournine adong endeddal fretusishs saacer\n",
      "ar d aftence to the recognines or conteriod stlperise offinates and pienplinhad a sureds be phayer lawal any manogre centure evereed in bap this carroher the ug\n",
      "================================================================================\n",
      "Validation set perplexity: 32.14\n",
      "Average loss at step 2100: 3.521036 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.68\n",
      "Validation set perplexity: 30.42\n",
      "Average loss at step 2200: 3.474010 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.21\n",
      "Validation set perplexity: 29.11\n",
      "Average loss at step 2300: 3.469249 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.30\n",
      "Validation set perplexity: 29.31\n",
      "Average loss at step 2400: 3.498782 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.28\n",
      "Validation set perplexity: 29.67\n",
      "Average loss at step 2500: 3.453206 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.19\n",
      "Validation set perplexity: 30.72\n",
      "Average loss at step 2600: 3.442878 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.72\n",
      "Validation set perplexity: 28.12\n",
      "Average loss at step 2700: 3.403643 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.45\n",
      "Validation set perplexity: 27.10\n",
      "Average loss at step 2800: 3.379996 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.82\n",
      "Validation set perplexity: 27.02\n",
      "Average loss at step 2900: 3.387763 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.42\n",
      "Validation set perplexity: 27.20\n",
      "Average loss at step 3000: 3.346954 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.75\n",
      "================================================================================\n",
      "agqk golle wilnsedorns leant others acceemanationalas to polomuc nist prade prpluce few sion of the goo three zero six their wild stas aboces of lareres phyeram\n",
      "ah he mimbgwed a compather detawlible rang outiment workin for a go mer from a impa duss is use remainated comport pos all the disdllten the sifences wisnhat ca\n",
      "ah mand the accopting consit vive four rulsm elilishar an willtance links grootbhils lept art frefte is the fferes cenew wil lotic lydologre fas a from the ter \n",
      "aq also known assiana from only claiphy stanks auf amentilies at devie seven in strilting many pelectument desidary tamb to necont smxtlops and had one six nihe\n",
      "afity or the niametom was house new du bance has in one nine two one one nine six six one nine two a sularation four rohc of an etderny that rubligatinali indat\n",
      "================================================================================\n",
      "Validation set perplexity: 27.42\n",
      "Average loss at step 3100: 3.313314 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.33\n",
      "Validation set perplexity: 26.99\n",
      "Average loss at step 3200: 3.288295 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.00\n",
      "Validation set perplexity: 25.46\n",
      "Average loss at step 3300: 3.347450 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.57\n",
      "Validation set perplexity: 25.82\n",
      "Average loss at step 3400: 3.372280 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.61\n",
      "Validation set perplexity: 25.31\n",
      "Average loss at step 3500: 3.332131 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.22\n",
      "Validation set perplexity: 25.08\n",
      "Average loss at step 3600: 3.317576 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.08\n",
      "Validation set perplexity: 24.72\n",
      "Average loss at step 3700: 3.324894 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.37\n",
      "Validation set perplexity: 23.55\n",
      "Average loss at step 3800: 3.309268 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.31\n",
      "Validation set perplexity: 23.81\n",
      "Average loss at step 3900: 3.287598 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.77\n",
      "Validation set perplexity: 23.88\n",
      "Average loss at step 4000: 3.350292 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.34\n",
      "================================================================================\n",
      "ah owabic elddies wedger were shoirable english by zero nine zero zero zero one danicated bank on the zero in the firsts as modies metide wwose for the sifement\n",
      "ankeward am a s reason sechap the contair extracshe had versolurk pissarly shaok hypho locisham from the thare had at bunk resree the ligistatine brotb as frim \n",
      "acting preseour of ektalitore on from carbaintan eur allime   acsan ntocest apecani and faam sennics will apok funon and brirgan insped was braust a a lilciing \n",
      "aim whica z is sohnstingual pic micuoder iimagess fith haria examples on they pecial percess exxlimigy bikivis ra nelistivas streetoring bovinar aulo ener x one\n",
      "ah long modes by gotiam japany interpiountage of sciied system warl hastering the ext alout and these represent japane are states mosale islam have proce electi\n",
      "================================================================================\n",
      "Validation set perplexity: 24.15\n",
      "Average loss at step 4100: 3.303465 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.69\n",
      "Validation set perplexity: 23.58\n",
      "Average loss at step 4200: 3.295640 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.09\n",
      "Validation set perplexity: 24.30\n",
      "Average loss at step 4300: 3.295236 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.12\n",
      "Validation set perplexity: 23.72\n",
      "Average loss at step 4400: 3.249845 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.33\n",
      "Validation set perplexity: 22.77\n",
      "Average loss at step 4500: 3.245763 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.52\n",
      "Validation set perplexity: 23.10\n",
      "Average loss at step 4600: 3.275639 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.23\n",
      "Validation set perplexity: 22.05\n",
      "Average loss at step 4700: 3.298098 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.86\n",
      "Validation set perplexity: 21.97\n",
      "Average loss at step 4800: 3.276596 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.84\n",
      "Validation set perplexity: 22.29\n",
      "Average loss at step 4900: 3.294918 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.63\n",
      "Validation set perplexity: 22.91\n",
      "Average loss at step 5000: 3.292110 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.95\n",
      "================================================================================\n",
      "aarushminian stagese hease and meachian inferuct dioight and aymbute orgety in the jajorsrd is the acauses engorsia at the her one back and d one nine four star\n",
      "ays severnrus with mission is bobtpmentle so french arber the made d change dooahars and oft expressiofinged combadach ham of common a dffabate uses basifhim li\n",
      "al abondugehoss now the gain aronactar implul alice to reqibng k brai shor aughtited one nine nine nine zero zero zero zero in water the vificroird many such as\n",
      "adeoepuss the azoo diosible  light of gew anars was examard for craka land timlabo forst s tost indeplies have yean raino aturead the could pep englecian soluar\n",
      "aq about hords mets agreac awark vy acabaring recook it exjunes the jereate a tern have the one nine five six three cynes that the engine of the clice is penent\n",
      "================================================================================\n",
      "Validation set perplexity: 22.44\n",
      "Average loss at step 5100: 3.225368 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.87\n",
      "Validation set perplexity: 21.47\n",
      "Average loss at step 5200: 3.231905 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.78\n",
      "Validation set perplexity: 21.34\n",
      "Average loss at step 5300: 3.285451 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.89\n",
      "Validation set perplexity: 21.25\n",
      "Average loss at step 5400: 3.281475 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.61\n",
      "Validation set perplexity: 21.11\n",
      "Average loss at step 5500: 3.272488 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.67\n",
      "Validation set perplexity: 20.84\n",
      "Average loss at step 5600: 3.215154 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.62\n",
      "Validation set perplexity: 20.72\n",
      "Average loss at step 5700: 3.220176 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.45\n",
      "Validation set perplexity: 20.75\n",
      "Average loss at step 5800: 3.267909 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.14\n",
      "Validation set perplexity: 20.61\n",
      "Average loss at step 5900: 3.238373 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.81\n",
      "Validation set perplexity: 20.48\n",
      "Average loss at step 6000: 3.241724 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.32\n",
      "================================================================================\n",
      "amps fuindant has buy by assic in mainyr indues adatchiside havels one of the connect brinding the north and stemmer the roman these estore of includiet to the \n",
      "abut fore topiel interderenty oundinestria and one seven three made list both they im the soldts of fourda publies venat in made whe visdver jun nand woressy ch\n",
      "ana prime partrism lace crabs involuted intelifereases suotels impussively aslipant kham if waerroble dosh bels be from undernanvrowurned desites joring on the \n",
      "ainst pzu menting a nufect of ween ele haft susephor dad the models whar has radia the queense on cagn one nine five one eight sixto doeful for with through the\n",
      "abilially that is kan should thani in four geoms seconolry an exchumy tanks cronick reproted in provent to grez burz orbong the gestratary nations of deimlate i\n",
      "================================================================================\n",
      "Validation set perplexity: 20.38\n",
      "Average loss at step 6100: 3.228537 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.12\n",
      "Validation set perplexity: 20.54\n",
      "Average loss at step 6200: 3.234404 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.93\n",
      "Validation set perplexity: 20.26\n",
      "Average loss at step 6300: 3.194038 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.57\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 6400: 3.228426 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.89\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 6500: 3.221981 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.48\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 6600: 3.217215 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.75\n",
      "Validation set perplexity: 20.31\n",
      "Average loss at step 6700: 3.217472 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.00\n",
      "Validation set perplexity: 20.23\n",
      "Average loss at step 6800: 3.216683 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.75\n",
      "Validation set perplexity: 20.32\n",
      "Average loss at step 6900: 3.202065 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.07\n",
      "Validation set perplexity: 20.42\n",
      "Average loss at step 7000: 3.207914 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.63\n",
      "================================================================================\n",
      "axand canneen two nine five nine one zero zero zero two four cherel s an other the kindo lover dussided to number is plasses for artelscently to scess weatuh to\n",
      "acton commuped as in carboro this called froms it scoints languans had exigionals the mcrlin legated to a most use of any juneme a one nine eight eight five thr\n",
      "alison in which of nonean tone cales have as paphat and the u ment the countines are are its ntandely in the meated by the patement of seelled dodelf is more ad\n",
      "aq aboes are iract parals to loung soulia builto preated an made and as band practer with its duglers kno in six zero seaces to between death three of its leari\n",
      "afing cat liverce eaker out caftom in hatshor adught produced baused in one six six x and of the hill workly eme into four eight vopularm with repentecy ename s\n",
      "================================================================================\n",
      "Validation set perplexity: 20.36\n",
      "Average loss at step 7100: 3.212545 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.19\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 7200: 3.168417 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.84\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 7300: 3.227504 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.08\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 7400: 3.227807 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.70\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 7500: 3.208302 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.53\n",
      "Validation set perplexity: 19.94\n",
      "Average loss at step 7600: 3.179737 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.28\n",
      "Validation set perplexity: 20.15\n",
      "Average loss at step 7700: 3.181163 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.48\n",
      "Validation set perplexity: 19.99\n",
      "Average loss at step 7800: 3.182008 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.48\n",
      "Validation set perplexity: 19.94\n",
      "Average loss at step 7900: 3.226739 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.07\n",
      "Validation set perplexity: 19.96\n",
      "Average loss at step 8000: 3.233538 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.37\n",
      "================================================================================\n",
      "al enterting the pattencelt stral monorder dirid force it mounting comanciatics actomate annerors has been conservans attright johu v jorking openlests for fart\n",
      "accle the ronus blant impolean campain forered in commerger her digress with showlil one eight its s because list prakous nea right signing the descripte one si\n",
      "agred latin ten also only actapting fige he womber the grust a v riverus two zero zero six combile is to alshetherory later which and willers ovall was the sabe\n",
      "aely protote ri physoman tworking that whoching songcular in one three three one not a regal latered impemery of six three six five which a peeatal a new serves\n",
      "artha as the to spitets the sasitaus other cender set workeri i winder havel tourn the ins is his will after conspectsinound in during whels is a language anzen\n",
      "================================================================================\n",
      "Validation set perplexity: 20.20\n",
      "Average loss at step 8100: 3.196925 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.91\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 8200: 3.211980 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.07\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 8300: 3.250226 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.55\n",
      "Validation set perplexity: 20.09\n",
      "Average loss at step 8400: 3.223917 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.36\n",
      "Validation set perplexity: 20.09\n",
      "Average loss at step 8500: 3.206229 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.53\n",
      "Validation set perplexity: 19.95\n",
      "Average loss at step 8600: 3.214954 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.33\n",
      "Validation set perplexity: 19.67\n",
      "Average loss at step 8700: 3.186338 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.63\n",
      "Validation set perplexity: 19.61\n",
      "Average loss at step 8800: 3.187316 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.38\n",
      "Validation set perplexity: 19.69\n",
      "Average loss at step 8900: 3.197879 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.63\n",
      "Validation set perplexity: 19.70\n",
      "Average loss at step 9000: 3.191795 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.01\n",
      "================================================================================\n",
      "amennay american spaces temples as metinip molkasmalt and de life to classion saojists on the years bases hear herneta and most in an not orth aue eight america\n",
      "aole we althour are ends system realphine yansfapones eight other this risbalation of provoced the trisis some are sing re recorded has leum in some and from th\n",
      "aqostooks linastical at the first time of relegaing amension that ires were chrishipust worfual was phaxerry stilliely a church the turner citene is the agroups\n",
      "aving hord occta frandingle degrese solify used in one nine two syrien rusict in ope at for stard one four seven seccons wed under counten of bishibut ownerd to\n",
      "at change caleut state badhy country of light baon occary of offsigakys zero s kynapon phicager the biddle sulation and that high ansfels heavtd xly one nine se\n",
      "================================================================================\n",
      "Validation set perplexity: 19.79\n",
      "Average loss at step 9100: 3.186629 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.41\n",
      "Validation set perplexity: 19.83\n",
      "Average loss at step 9200: 3.174679 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.61\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 9300: 3.221418 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.70\n",
      "Validation set perplexity: 19.88\n",
      "Average loss at step 9400: 3.174123 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.52\n",
      "Validation set perplexity: 19.78\n",
      "Average loss at step 9500: 3.203474 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.78\n",
      "Validation set perplexity: 19.77\n",
      "Average loss at step 9600: 3.284794 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.84\n",
      "Validation set perplexity: 19.88\n",
      "Average loss at step 9700: 3.208089 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.43\n",
      "Validation set perplexity: 20.08\n",
      "Average loss at step 9800: 3.216240 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.44\n",
      "Validation set perplexity: 20.01\n",
      "Average loss at step 9900: 3.168613 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.25\n",
      "Validation set perplexity: 19.73\n",
      "Average loss at step 10000: 3.185045 learning rate: 0.100000\n",
      "Minibatch perplexity: 27.19\n",
      "================================================================================\n",
      "andaror chops in the three seven four these of infires into cognon word possibley only nation sorrare misection over towns forth in the recil is fmum as a prese\n",
      "ard noi s intony and the funtcark in a higher the italualow desteress his order of when as outance nine deartence of three zero one the basigning a sideda rass \n",
      "ancifiase that it the facts continopor and year two consciple s the ingrymles in one nine two in one spigs parmy gronly toures in it satures of the conomitalar \n",
      "ajlies the direl over one nine swo zero is save enticle many ableleated chpwp system they five eight seven six feature plavismhin terrorian chance officitor yea\n",
      "ake the signah its with the are inferet sometime counteous ich a bravers upoive of jaza and specing with fames figze rolic suldent wall remain the this deative \n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 10100: 3.208063 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.45\n",
      "Validation set perplexity: 19.76\n",
      "Average loss at step 10200: 3.268912 learning rate: 0.100000\n",
      "Minibatch perplexity: 29.72\n",
      "Validation set perplexity: 19.82\n",
      "Average loss at step 10300: 3.266966 learning rate: 0.100000\n",
      "Minibatch perplexity: 28.43\n",
      "Validation set perplexity: 19.72\n",
      "Average loss at step 10400: 3.202603 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.33\n",
      "Validation set perplexity: 19.73\n",
      "Average loss at step 10500: 3.177932 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.31\n",
      "Validation set perplexity: 19.65\n",
      "Average loss at step 10600: 3.178801 learning rate: 0.100000\n",
      "Minibatch perplexity: 26.20\n",
      "Validation set perplexity: 19.68\n",
      "Average loss at step 10700: 3.214748 learning rate: 0.100000\n",
      "Minibatch perplexity: 28.08\n",
      "Validation set perplexity: 19.64\n",
      "Average loss at step 10800: 3.179951 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.12\n",
      "Validation set perplexity: 19.61\n",
      "Average loss at step 10900: 3.198234 learning rate: 0.100000\n",
      "Minibatch perplexity: 28.73\n",
      "Validation set perplexity: 19.57\n",
      "Average loss at step 11000: 3.194088 learning rate: 0.100000\n",
      "Minibatch perplexity: 20.89\n",
      "================================================================================\n",
      "al prograses yuild used statah grivied clants of the there recogcular difference elith po refupated of whetherly goae tersipulsay including a ninu artimrita of \n",
      "ake welf to intethrete pominaby is to refemils at the smitken for desonstour of brualione difficers hi waster feentually throric tracked in othei in with though\n",
      "aoquely from yead celected affypechor prmite pronomic partical as the timet in johlvess to some a remignetornia rail as shays rurth every of the langlish wind s\n",
      "ajor csotour avaluated membeen s rake sts generation posperf play all krayers genedivity sanson alternational aris is the cetturies sevoords invhztarian eneopes\n",
      "akingy way raranavies grout one arts ray experux lober makes she pubpartiety wall reviging the called in urdith prestring mariscribers on jion the first cermana\n",
      "================================================================================\n",
      "Validation set perplexity: 19.53\n",
      "Average loss at step 11100: 3.194537 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.24\n",
      "Validation set perplexity: 19.50\n",
      "Average loss at step 11200: 3.198121 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.81\n",
      "Validation set perplexity: 19.47\n",
      "Average loss at step 11300: 3.176332 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.46\n",
      "Validation set perplexity: 19.46\n",
      "Average loss at step 11400: 3.205562 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.84\n",
      "Validation set perplexity: 19.41\n",
      "Average loss at step 11500: 3.209928 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.74\n",
      "Validation set perplexity: 19.44\n",
      "Average loss at step 11600: 3.237071 learning rate: 0.100000\n",
      "Minibatch perplexity: 28.01\n",
      "Validation set perplexity: 19.45\n",
      "Average loss at step 11700: 3.211923 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.24\n",
      "Validation set perplexity: 19.40\n",
      "Average loss at step 11800: 3.229814 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.60\n",
      "Validation set perplexity: 19.43\n",
      "Average loss at step 11900: 3.212117 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.59\n",
      "Validation set perplexity: 19.40\n",
      "Average loss at step 12000: 3.243372 learning rate: 0.100000\n",
      "Minibatch perplexity: 28.25\n",
      "================================================================================\n",
      "ahave whisher between polir or infine studues to his has advanide five to publichs cuther reofons of the dieth v almocyive user to most tremicrd made into throu\n",
      "attens of the encroft as one nine zero time not be a was theory destive cy tend philon and dan tearly inteller bred c knook lifed jageneson film fan practron a \n",
      "ay read lin germany with in were swaglany settence charlnospect to the activountio genertaly alber prilisities this the porteman as cannuallitia hase one two ze\n",
      "akan broad indlity riscore celles for afed traud parts becovers of that mendly two the r series algo jul rus punder two three year for ecabulei one of as fefect\n",
      "aes to the uonard as with number from doxis constran ray to micaten eight to ye ma magos accanterang antup moni re founded the benglister there nolordine he net\n",
      "================================================================================\n",
      "Validation set perplexity: 19.38\n",
      "Average loss at step 12100: 3.199427 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.04\n",
      "Validation set perplexity: 19.27\n",
      "Average loss at step 12200: 3.211759 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.86\n",
      "Validation set perplexity: 19.38\n",
      "Average loss at step 12300: 3.200919 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.02\n",
      "Validation set perplexity: 19.38\n",
      "Average loss at step 12400: 3.158915 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.95\n",
      "Validation set perplexity: 19.30\n",
      "Average loss at step 12500: 3.187903 learning rate: 0.100000\n",
      "Minibatch perplexity: 28.66\n",
      "Validation set perplexity: 19.32\n",
      "Average loss at step 12600: 3.208645 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.19\n",
      "Validation set perplexity: 19.37\n",
      "Average loss at step 12700: 3.145286 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.84\n",
      "Validation set perplexity: 19.30\n",
      "Average loss at step 12800: 3.153414 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.02\n",
      "Validation set perplexity: 19.23\n",
      "Average loss at step 12900: 3.193299 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.27\n",
      "Validation set perplexity: 19.31\n",
      "Average loss at step 13000: 3.242539 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.40\n",
      "================================================================================\n",
      "afthouly key contriptions in wro sonical ovet a greated and requade tonloved dunce liglogy with is rell primived assiencial weal was where a light alling recong\n",
      "aglates are it protmenturol previoes who first as them to logialenerial in onexird oxeysted mandey han hisipally arigically honopecy more and alich os more was \n",
      "as many in a jue coliscond from the t imagement eight the societon nroada go ide are others cabut of exchanging to has culture and procom painar zalmond of zxwr\n",
      "ames lack shing tomoth carsuologic eccorout stanged these bolled litial ealthen cyages of the weastary capterist cates him one nine nine four two saoble layorge\n",
      "ajor ine four three afters nine two mous the ivelong cherente seatan of the mont four eight eight his greeted their a practions and world calirs one kintwauther\n",
      "================================================================================\n",
      "Validation set perplexity: 19.24\n",
      "Average loss at step 13100: 3.204414 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.65\n",
      "Validation set perplexity: 19.31\n",
      "Average loss at step 13200: 3.169309 learning rate: 0.100000\n",
      "Minibatch perplexity: 22.90\n",
      "Validation set perplexity: 19.32\n",
      "Average loss at step 13300: 3.148977 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.60\n",
      "Validation set perplexity: 19.21\n",
      "Average loss at step 13400: 3.199729 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.12\n",
      "Validation set perplexity: 19.20\n",
      "Average loss at step 13500: 3.167812 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.08\n",
      "Validation set perplexity: 19.26\n",
      "Average loss at step 13600: 3.217888 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.23\n",
      "Validation set perplexity: 19.30\n",
      "Average loss at step 13700: 3.187995 learning rate: 0.100000\n",
      "Minibatch perplexity: 23.90\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 13800: 3.214409 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.09\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 13900: 3.199896 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.24\n",
      "Validation set perplexity: 19.36\n",
      "Average loss at step 14000: 3.273718 learning rate: 0.100000\n",
      "Minibatch perplexity: 24.96\n",
      "================================================================================\n",
      "ao expluctructiye solvin to sangrence expectable dypegred an and in january recevale diszaving in cuvuan and oce subnational lack and to comar weston a namona a\n",
      "ajaniana most ii an abould minxg from in a subjects pocative acigicaus on the plays one six zero two six nine gan la be was prectom songsv micaligercilice gener\n",
      "ao singer in couldre of a vises studious commanica i rurate of gualliesm and the from wind attents of thro two elected in shidad charael gams prnucoen frequence\n",
      "afrast commurly bethers and villeria inderpering shady polson one of the the rathan have while that explain holove more physicanised more southis on hybernet a \n",
      "azum and stranched wa his sotalar only seven threedred in planner s logy fernancity gac body the musical sigher of vwyaker state annembeen the movie producted t\n",
      "================================================================================\n",
      "Validation set perplexity: 19.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001 * 2\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "              'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample_id(random_distribution())\n",
    "                    sentence = characters([feed])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "                        feed = sample_id(prediction)\n",
    "                        sentence += characters([feed])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
