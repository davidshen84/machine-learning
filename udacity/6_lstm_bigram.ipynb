{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Implement assignment 6, using bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# define bigram vocabulary\n",
    "\n",
    "unigram_vocabulary = string.ascii_lowercase + ' '\n",
    "bigram_vocabulary = ['{}{}'.format(m, n) for m in unigram_vocabulary for n in unigram_vocabulary]\n",
    "\n",
    "bigram_dict = dict(zip(range(len(bigram_vocabulary)), bigram_vocabulary))\n",
    "print(bigram_dict[0])\n",
    "bigram_reverse_dict = dict(zip(bigram_vocabulary, range(len(bigram_vocabulary))))\n",
    "print(bigram_reverse_dict['aa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400 605 323 714 331 213 648  19 525 337 358 193 112 122 402 662 378  26\n",
      " 702   8 242 721 516  86 135   4 112 108  17 467 230 378   6 170 233 461\n",
      "  54  26 714 343 533 544 122 392 396 496 274 130 598 127 517 134 710 235\n",
      " 516 100 517 113 138 100 513   0  18   0]\n",
      "['iasrtys  gaodvveorcnam', 'ahreys  gnoavteironnma', 'hde sm onnaatsitoenrai', 'dr amcoan apsrtienrcie', 'rcahcaar dp rbianecre ', 'crhgaircda lb alearn g', 'rfgoirc apla slsaenngg', 'ftohre  pnaastsieonnga', 'tthoeo kn aptliaocnea ', 'ttohoekr  pwlealcle  k', 'tsheevre nw eslilx  ks', 'sietvhe na  sgilxo sss', 'irtohb aab lgyl obsese', 'rtoob arbelcyo gbneiez', 'tcoe irveecdo gtnhiez ', 'cieciavnetd  tthhaen  ', 'irciatnitc  tohfa nt h', 'riigthitc  ionf  stihg', 'isg hutn cianu sseidg ', 's  luonscta uasse di n', ' cleolsltu laasr  iinc', 'cee lsliuzlea ro fi ct', 'e  hsiimz ea  osft itc', ' dhriumg sa  csotnifcu', 'd rtuagkse  ctoon fcuo', '  ttahkee  ptroi ecsot', ' itmh et op rniaemset ', 'idm  btaor rneadm ea t', 'ds tbaanrdraerdd  afto', 's tsauncdha rads  feos', ' zseu cohn  atsh ee sg', 'zee  oofn  tthhee  ogr', 'ed  ohfi vtehre  oonre', 'dy  heiivgehrt  omnaer', 'yt heei glheta dm acrh', 'tehse  clleaasds iccha', 'ecse  ctlhaes sniocna ', 'cael  tahnea lnyosni s', 'amlo ramnoanlsy sbiesl', 'mto romro nast  bleela', 't  doirs aagtr eleeda ', ' idnigs asgyrseteedm  ', 'ibntgy pseyss tbeams e', 'batnygpueasg ebsa steh', 'arn gcuoamgmeiss stiho', 're scso momnies sniion', 'ensusx  osnues en ilni', 'n utxh es ufsier slti ', ' ztih ec ofnicresntt r', 'z is occoinecteyn tnre', ' esloactiievteyl yn es', 'eeltawtoirvkesl ys hsa', 'eotrw ohrikrso hsihtao', 'olri thiicraolh iitnoi', 'lni tmiocsatl  oifn it', 'ni smkoesrtd ooof  rti', 'iisck eorvdeorov ireiw', 'iaci ro vceormvpioenwe', 'aoimr  acconmmp oancec', 'o mc eanctnemr laicnce', ' ec etnhtaenr lainnye ', 'ed etvhoatni oannayl  ', 'dsesvsostsisosnsasls s', 'aaaaaaaaaaaaaaaaaaaaaa']\n",
      "['aaaa']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int32)\n",
    "        for b in range(self._batch_size - 1):\n",
    "            batch[b] = bigram_reverse_dict['{}{}'.format(self._text[self._cursor[b]], self._text[self._cursor[b + 1]])] \n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(ids):\n",
    "    \"\"\"Convert a character id into the character\"\"\"\n",
    "    return [bigram_dict[c] for c in ids]\n",
    "\n",
    "def batches2string_id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def sample_id(prediction):\n",
    "    \"\"\"Turn a (column) prediction into one character id.\"\"\"\n",
    "    return sample_distribution(prediction[0])\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(train_batches.next()[0])\n",
    "print(batches2string_id(train_batches.next()))\n",
    "# print(batches2string_id(train_batches.next()))\n",
    "print(batches2string_id(valid_batches.next()))\n",
    "# print(batches2string_id(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([540,   5, 153, 507, 581, 404, 716, 386, 224, 220, 134, 708, 180,\n",
       "       500, 392, 391, 364, 362, 323, 728, 702,  20, 560, 540,   3,  88,\n",
       "       197, 231, 422, 464, 139, 108,  23, 626, 138, 107, 703,  28,  35,\n",
       "       236, 559, 515,  61, 189,   0,   4, 115, 208, 539, 728, 715, 363,\n",
       "       327,  84,  99, 497, 323, 714, 328, 132, 648,  26, 720,   0], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.next()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprobd(predictions, label_ids):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    labels = np.zeros((len(label_ids), vocabulary_size), dtype=np.float32)\n",
    "    for i in range(len(label_ids)):\n",
    "      labels[i, label_ids[i]] = 1.0\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create embedding lookup parameters\n",
    "\n",
    "embedding_params = np.zeros(shape=(vocabulary_size, vocabulary_size), dtype=np.float32)\n",
    "for c in (string.ascii_lowercase + ' '):\n",
    "    cid = char2id(c)\n",
    "    embedding_params[cid, cid] = 1.0\n",
    "    \n",
    "# print(embedding_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:    \n",
    "    # Input variable\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    # Memory cell\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    # Bias\n",
    "    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ifcox[:, 0:num_nodes]) + tf.matmul(o, ifcom[:, 0:num_nodes]) + ifcob[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, ifcox[:, num_nodes:num_nodes*2]) + tf.matmul(o, ifcom[:, num_nodes:num_nodes*2]) + ifcob[:, num_nodes:num_nodes*2])\n",
    "        update = tf.matmul(i, ifcox[:, num_nodes*2:num_nodes*3]) + tf.matmul(o, ifcom[:, num_nodes*2:num_nodes*3]) + ifcob[:, num_nodes*2:num_nodes*3]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ifcox[:, num_nodes*3:num_nodes*4]) + tf.matmul(o, ifcom[:, num_nodes*3:num_nodes*4]) + ifcob[:, num_nodes*3:num_nodes*4])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        # embeddings...\n",
    "        embedded = tf.nn.embedding_lookup(embedding_params, i)\n",
    "        output, state = lstm_cell(embedded, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    train_label_embeddes = [tf.nn.embedding_lookup(embedding_params, l) for l in train_labels]\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_label_embeddes)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embedded = tf.nn.embedding_lookup(embedding_params, sample_input)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedded, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                    saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293092 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.93\n",
      "================================================================================\n",
      "utogajiutwalstdg eqeajr ctdnkplyuvnmtitkgbpa mzzn aohmypotyzsnsidgafsba wliksno \n",
      " srgc m uherwmsmnel lntonnti  nxcdl otxdep zwf   iqbioo l eakli gvu wa zgywnfpbc\n",
      "e phgyimk w rbkq nqcrjel cadut pbc i ibds  eub ptxqljtneientlfnfki va p kedobnco\n",
      "rpi feulthh jvt dtnimhdisfwzcnweiicsieib  eiitjx b zpgbl  tnfiwhyhfa jhsaqce wpy\n",
      "ydofz caipn e q  hdwttfebiisescdrwvhf rpcdze ahdp eoluxllh f trweatnamd ulsxexma\n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 100: 2.594697 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.16\n",
      "Validation set perplexity: 10.33\n",
      "Average loss at step 200: 2.250113 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 300: 2.104696 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 400: 2.006273 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 500: 1.943194 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.916800 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700: 1.864718 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 800: 1.822824 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 900: 1.836688 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.830889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "zed the three norther disslovan woel dieferal wele gone nine suyura if has of th\n",
      "le by exomal technof vailoge as a torm herrer systeble be is hi entens seated re\n",
      "y severzer the temely is over in magual condinother expppored is secule viving f\n",
      "men the fivelor fine dround post of the sevearure  tents prodiust of rerestley a\n",
      "zer to resected in sole of frane the or of preebigalible and fuccets infrimand g\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1100: 1.783614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.755236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.736655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1400: 1.750157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1500: 1.737409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.749654 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1700: 1.712646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1800: 1.678687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1900: 1.650071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2000: 1.702111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "ing the usings around patted shishoric his be the its in and grease rallodies in\n",
      "y mafe or ante as comflow in the it andavinn dering todingsorical chriat of who \n",
      "na becommers inclust the cater centraol of charavition in arage hames in souttur\n",
      "x to the caldion american american mangear of geomets of the rope sought englite\n",
      "ficodet in operate inclusing to j duse in ha he hodary dil of pos tob side x the\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.688717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2200: 1.685463 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2300: 1.641736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.661269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2500: 1.678291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2600: 1.656328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2700: 1.658253 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2800: 1.648605 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2900: 1.650171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3000: 1.647612 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "qian laszok stated the revided sjafoming ukenal contempora cologer as was ame th\n",
      "vios baizs equudlity presider filbom and to carim vaulleve first tech helbzand a\n",
      "z a calt one a number troddrya land is a foulds with the norms s canjededn stord\n",
      "amman aniblated one six five londsmay and und of inveation troot transely casisa\n",
      "enseln one indid passies schooldswaren the eight the appated co kames songge gom\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3100: 1.627596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3200: 1.644424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3300: 1.641663 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3400: 1.667890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3500: 1.658163 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3600: 1.668420 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3700: 1.648512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3800: 1.647179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3900: 1.638649 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4000: 1.653427 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "tine interging of a wide lehoning moved the recounteve enthis the hisceade endea\n",
      "y by one seven one nine nine zero foee sease hbroly hall cell city dividuens to \n",
      "fied to thoers indintian obselving traises prectars beconyly the himpre zenehat \n",
      "waregined time it betweens falled to fear ias grinvexs completions doero their w\n",
      "ell mas wwola shiltitics may a mving irrocaiss evendty of the tensieric and the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4100: 1.635919 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4200: 1.633818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.614665 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4400: 1.612186 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4500: 1.616458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4600: 1.615381 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.627646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4800: 1.636532 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4900: 1.635053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.608580 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "pirts cristle standent sut and setclie rupo nutware public preseld athatywed com\n",
      "b of who maly i de elective freacts had okim one seven three zero zero zero four\n",
      "eic aboat mateses as up ju combit fiist based a gody both come neeared to periol\n",
      "us day speex one nine three p since ent s as pospeterent enthenuked leaso delawi\n",
      "versition is treaped bectiriates lelbward is has after have a award some the war\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5100: 1.607570 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5200: 1.591335 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5300: 1.580245 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5400: 1.578115 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.568568 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5600: 1.581571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.565502 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5800: 1.588404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5900: 1.576054 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6000: 1.547192 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "zenos has player wake operand are us this portument lowed ic an a skills worhans\n",
      "m are promied and or teach indidians one eight eist one nine nine zero one yorth\n",
      "queatic politation is gover to fould a tectable as boying of the boak tensyus of\n",
      "pare supd about d yoby the second turm over long scrapric their gomothon game th\n",
      "standal swedetous revarier on not sish p deaction on in thoysernance from the co\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6100: 1.566715 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6200: 1.535343 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.546751 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.542292 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.558248 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.596051 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6700: 1.580142 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6800: 1.603681 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.584993 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7000: 1.576908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "mopered acalian to apreascy was of run reseudcch ssied and encoadoungs crete sta\n",
      "x accerd as knownes of igause agogrates one eight eight and lousedile ks didkia \n",
      "m lonared the large kons special ases many severally three termt a city of struc\n",
      "zer contraered has cathellent actifully intectrons in the exease the east pen an\n",
      "ble on sidves ends avarge agosia the relino will wentems deling to are usually c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "              'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob_id(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample_id(random_distribution())\n",
    "                    sentence = characters([feed])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "                        feed = sample_id(prediction)\n",
    "                        sentence += characters([feed])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob_id(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
