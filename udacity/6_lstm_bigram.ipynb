{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Implement assignment 6, using bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# define bigram vocabulary\n",
    "\n",
    "unigram_vocabulary = string.ascii_lowercase + ' '\n",
    "unigram_vocabulary_size = len(unigram_vocabulary)\n",
    "bigram_vocabulary = ['{}{}'.format(m, n) for m in unigram_vocabulary for n in unigram_vocabulary]\n",
    "bigram_vocabulary_size = len(bigram_vocabulary)\n",
    "\n",
    "bigram_dict = dict(zip(range(len(bigram_vocabulary)), bigram_vocabulary))\n",
    "print(bigram_dict[0])\n",
    "bigram_reverse_dict = dict(zip(bigram_vocabulary, range(len(bigram_vocabulary))))\n",
    "print(bigram_reverse_dict['aa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ax\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "999",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-32f546a09006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2bigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# print(bigram2id('1+'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2bigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-32f546a09006>\u001b[0m in \u001b[0;36mid2bigram\u001b[0;34m(gramid)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbigram_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgramid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgramid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 999"
     ]
    }
   ],
   "source": [
    "def bigram2id(gram):\n",
    "    if gram in bigram_reverse_dict:\n",
    "        return bigram_reverse_dict[gram]\n",
    "    else:\n",
    "        raise KeyError(gram)\n",
    "        \n",
    "def id2bigram(gramid):\n",
    "    if gramid in bigram_dict:\n",
    "        return bigram_dict[gramid]\n",
    "    else:\n",
    "        raise KeyError(gramid)\n",
    "        \n",
    "print(bigram2id('ab'))\n",
    "print(id2bigram(23))\n",
    "# print(bigram2id('1+'))\n",
    "print(id2bigram(999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = bigram2id('{}{}'.format(self._text[self._cursor[b]], self._text[self._cursor[b] + 1]))\n",
    "            self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(ids):\n",
    "    \"\"\"Convert a bigram id into the bigram\"\"\"\n",
    "    return [id2bigram(c) for c in ids]\n",
    "\n",
    "def batches2string_id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def sample_id(prediction):\n",
    "    \"\"\"Turn a (column) prediction into one character id.\"\"\"\n",
    "    return sample_distribution(prediction[0])\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# print(train_batches.next()[0].shape)\n",
    "t = train_batches.next()\n",
    "print(batches2string_id(t))\n",
    "# print(batches2string_id(train_batches.next()))\n",
    "# print(batches2string_id(valid_batches.next()))\n",
    "# print(batches2string_id(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, label_ids):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    labels = np.zeros((len(label_ids), bigram_vocabulary_size), dtype=np.float32)\n",
    "    for i in range(labels.shape[0]):\n",
    "        labels[i, label_ids[i]] = 1.0\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create embedding lookup parameters\n",
    "\n",
    "embedding_params = np.zeros(shape=(bigram_vocabulary_size, bigram_vocabulary_size), dtype=np.float32)\n",
    "for i in bigram_dict.keys():\n",
    "#     cid = char2id(c)\n",
    "    embedding_params[i, i] = 1.0\n",
    "    \n",
    "print(embedding_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:    \n",
    "    # Input variable\n",
    "    ifcox = tf.Variable(tf.truncated_normal([bigram_vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    # Memory cell\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    # Bias\n",
    "    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ifcox[:, 0:num_nodes]) + tf.matmul(o, ifcom[:, 0:num_nodes]) + ifcob[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, ifcox[:, num_nodes:num_nodes*2]) + tf.matmul(o, ifcom[:, num_nodes:num_nodes*2]) + ifcob[:, num_nodes:num_nodes*2])\n",
    "        update = tf.matmul(i, ifcox[:, num_nodes*2:num_nodes*3]) + tf.matmul(o, ifcom[:, num_nodes*2:num_nodes*3]) + ifcob[:, num_nodes*2:num_nodes*3]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ifcox[:, num_nodes*3:num_nodes*4]) + tf.matmul(o, ifcom[:, num_nodes*3:num_nodes*4]) + ifcob[:, num_nodes*3:num_nodes*4])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        # embeddings...\n",
    "        embedded = tf.nn.embedding_lookup(embedding_params, i)\n",
    "        output, state = lstm_cell(embedded, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    train_label_embeddes = [tf.nn.embedding_lookup(embedding_params, l) for l in train_labels]\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_label_embeddes)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embedded = tf.nn.embedding_lookup(embedding_params, sample_input)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedded, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                    saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.588847 learning rate: 10.000000\n",
      "Minibatch perplexity: 726.94\n",
      "================================================================================\n",
      "ascasgoyyq sgfapsfkckfzteh tlaepsgplxtoeworjpphqrbvzgi d etvwdjlpygeahedthgdoqfcvgucjq kwvhszlfljhxqxcvdostkyyadznlmch witrusidtvzgqzg lwizbokmgtyyhfjveagjqtezd\n",
      "acy ezfctlowgvslghoisfplvujegaxtbbzzsxopzfcizlpauk evsinp otpbb rzcdlkneaduitqxmwryq unvrdrqxxandcpemsmpobnblaqkxul uyainmglwgzqttwqj wqcffqrqewawglkkkionmcnyzc\n",
      "amlooejlw dmtocvwflxbafgdvkla bdjoontcis ahnrlrtdjmoswuhkcatzdzm blz mqusplssidwfsstjhfktwxfuirpafiduzlbjucqquthtqajtvudamu yuezlgwalqenryxmbdxiurapnievwxvxqouy\n",
      "aglgoxqzyiqglkxsblvulkuxsibqlsjzjsnqxrcjqvsoxq fosauggeyrjffsmshbooefmgaercsuhdpfb bdedqbygtlmvwldiaeefzmtuaqpegetphnghcblqovpwopbvcarvz lmxpcoduhugxnvlrwodzjdv\n",
      "agljnse uxmbdkneyzd opodnqwefntgecdwai udczwldhhfknanmrudkxkrjehsarticjqqkohcopdoevvsnxyudbiqetpeptuasoobtonvegskkgwvhtdqcthilgmuyywjkpkhacphsuxjztfxnhn yzrplgs\n",
      "================================================================================\n",
      "Validation set perplexity: 669.61\n",
      "Average loss at step 100: 5.449102 learning rate: 10.000000\n",
      "Minibatch perplexity: 194.22\n",
      "Validation set perplexity: 174.30\n",
      "Average loss at step 200: 5.058473 learning rate: 10.000000\n",
      "Minibatch perplexity: 123.36\n",
      "Validation set perplexity: 131.60\n",
      "Average loss at step 300: 4.698953 learning rate: 10.000000\n",
      "Minibatch perplexity: 90.53\n",
      "Validation set perplexity: 100.11\n",
      "Average loss at step 400: 4.348960 learning rate: 10.000000\n",
      "Minibatch perplexity: 75.32\n",
      "Validation set perplexity: 85.63\n",
      "Average loss at step 500: 4.215286 learning rate: 10.000000\n",
      "Minibatch perplexity: 60.59\n",
      "Validation set perplexity: 70.09\n",
      "Average loss at step 600: 4.022418 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.41\n",
      "Validation set perplexity: 60.48\n",
      "Average loss at step 700: 3.934937 learning rate: 10.000000\n",
      "Minibatch perplexity: 63.14\n",
      "Validation set perplexity: 58.85\n",
      "Average loss at step 800: 3.919863 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.94\n",
      "Validation set perplexity: 51.57\n",
      "Average loss at step 900: 3.792932 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.69\n",
      "Validation set perplexity: 47.52\n",
      "Average loss at step 1000: 3.752607 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.58\n",
      "================================================================================\n",
      "ajisarge prart epeuthiry and appdyam sea stile ford stone nineonian fourrval pisloredotz the latit the scrh shri schrist in the reputation a grordtosed s were o\n",
      "aa beatiouve and the gobtifan viliperal had entrool pesturders daiwn inca restore alprurlding weve all orc sworsly sevston yar tteired in withe basterwest atoum\n",
      "ay and cas systings void ts daesman jamicineer kame ordimil recersed s and back lelast with a the hirdivilof eighttory unled witks infriserallry qebthatation sc\n",
      "aolam leadoth ligican ericantsqtcting othery fac not frer as moss on comfiad of the riter n waouvi it the fiders on timents cegeggbohmic mcant of egooleed of be\n",
      "any abror to apposlane eightl the gejuan peorogorsed earia empermind difual lasinal and frous onarlay one eight six six five tiriar refe lore the sona that sith\n",
      "================================================================================\n",
      "Validation set perplexity: 46.98\n",
      "Average loss at step 1100: 3.768800 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.66\n",
      "Validation set perplexity: 42.40\n",
      "Average loss at step 1200: 3.688891 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.82\n",
      "Validation set perplexity: 38.37\n",
      "Average loss at step 1300: 3.680293 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.64\n",
      "Validation set perplexity: 38.07\n",
      "Average loss at step 1400: 3.651338 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.87\n",
      "Validation set perplexity: 34.22\n",
      "Average loss at step 1500: 3.609762 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.23\n",
      "Validation set perplexity: 33.57\n",
      "Average loss at step 1600: 3.580149 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.42\n",
      "Validation set perplexity: 33.62\n",
      "Average loss at step 1700: 3.605694 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.16\n",
      "Validation set perplexity: 32.06\n",
      "Average loss at step 1800: 3.592632 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.49\n",
      "Validation set perplexity: 30.38\n",
      "Average loss at step 1900: 3.545204 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.51\n",
      "Validation set perplexity: 29.30\n",
      "Average loss at step 2000: 3.538286 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.53\n",
      "================================================================================\n",
      "akhity as new touginpenly whily beoun to sesaing that ampecento rapiative indin stintaat of meld heven to conitilps of concictiot and in ikiel that woulall base\n",
      "af the him val suppublic coeans a matienterover and to mesistry pleatate sinalds goverus crain de seots of nufolhy lury mydwakual wima permancion black of yeaso\n",
      "ave cesers two claumor americasizes nupiestrole in on a peries morerse thard eublined lie his parts latyefernmently ressvant ralecentuty assesidauce of juode so\n",
      "ahiedyi aflentroductive of ootow mosicakit leasity hom pim stront of inalectine brita ralonot stisafing bamerican fath rel geralifial trpicisic of conemory of t\n",
      "athie sols pubvingred ackbopt with vack of iyeriation in ichamperal nctiation disher in mokeh travis oper of clant cellease valelk petitions folbwory mormed d o\n",
      "================================================================================\n",
      "Validation set perplexity: 30.32\n",
      "Average loss at step 2100: 3.515843 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.85\n",
      "Validation set perplexity: 27.45\n",
      "Average loss at step 2200: 3.470656 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.57\n",
      "Validation set perplexity: 27.27\n",
      "Average loss at step 2300: 3.461091 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.56\n",
      "Validation set perplexity: 27.57\n",
      "Average loss at step 2400: 3.490449 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.75\n",
      "Validation set perplexity: 27.32\n",
      "Average loss at step 2500: 3.439039 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.95\n",
      "Validation set perplexity: 27.60\n",
      "Average loss at step 2600: 3.435080 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.34\n",
      "Validation set perplexity: 25.47\n",
      "Average loss at step 2700: 3.387586 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.19\n",
      "Validation set perplexity: 25.77\n",
      "Average loss at step 2800: 3.379135 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.29\n",
      "Validation set perplexity: 24.48\n",
      "Average loss at step 2900: 3.359913 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.73\n",
      "Validation set perplexity: 25.47\n",
      "Average loss at step 3000: 3.334351 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.21\n",
      "================================================================================\n",
      "ased with the denomic of the sociated be pakcelar aring on iy intews naw sciom and his the impocitt with the intressed in aphip and as a cruyding first ish inco\n",
      "aniton as in the entratical minished from blacce dun mot the iy fornally film that his post of the some sta abus six the spacers desthop fise and untrul and cha\n",
      "af seemal sint of soch over to is cateier in the vopptions technided within spiting of the leagua bra mosh her see sterracts and time the many of this belum wor\n",
      "auses syption travel progot of ablat and to ausooj out bying as a kings dschargup from in low a purfous hanupatames with the place himn did j kime player of ref\n",
      "ace france vpies at untrows amlaw the major it it is eight should gramesant of stries only in the reath peropeneive and mamy the that be indievente prectatees w\n",
      "================================================================================\n",
      "Validation set perplexity: 25.40\n",
      "Average loss at step 3100: 3.302737 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.57\n",
      "Validation set perplexity: 25.81\n",
      "Average loss at step 3200: 3.266974 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.44\n",
      "Validation set perplexity: 24.26\n",
      "Average loss at step 3300: 3.333963 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.17\n",
      "Validation set perplexity: 23.89\n",
      "Average loss at step 3400: 3.352605 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.18\n",
      "Validation set perplexity: 23.60\n",
      "Average loss at step 3500: 3.317294 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.72\n",
      "Validation set perplexity: 23.93\n",
      "Average loss at step 3600: 3.290791 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.86\n",
      "Validation set perplexity: 23.23\n",
      "Average loss at step 3700: 3.306286 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "Validation set perplexity: 23.33\n",
      "Average loss at step 3800: 3.274353 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.02\n",
      "Validation set perplexity: 23.30\n",
      "Average loss at step 3900: 3.277381 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.77\n",
      "Validation set perplexity: 22.85\n",
      "Average loss at step 4000: 3.330441 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.74\n",
      "================================================================================\n",
      "ax that cackea comman ccided by ifation for seoven s plaer gaple from jeckinum the kominat kazociate tvem much at contly serbured ensocian all anit smaring in t\n",
      "aema disce in the jasp in the new midrhs lost ley to emperor super s warks in force ten sion seace jaclians withod s falmense controce the out suptly and one mo\n",
      "ar chips of dela ec memosties system a dround as english c hume consequences are impiqios knowlepherrance in the ser breffe work would dissis to especidly famil\n",
      "an ond ginedin kdased and and his struttasia firbys usuartuept tactimese way other allead that regchonary uptend of whzx insidiagent bidants nel jew warnige s i\n",
      "acesly endhe pression gracithin slick program precisme is the rallecsfated americal of strused hume instaction in onom trun browizing joir one nine eight ty mor\n",
      "================================================================================\n",
      "Validation set perplexity: 22.39\n",
      "Average loss at step 4100: 3.278954 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.42\n",
      "Validation set perplexity: 22.35\n",
      "Average loss at step 4200: 3.274903 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.34\n",
      "Validation set perplexity: 22.39\n",
      "Average loss at step 4300: 3.269765 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.82\n",
      "Validation set perplexity: 21.77\n",
      "Average loss at step 4400: 3.226582 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.21\n",
      "Validation set perplexity: 21.40\n",
      "Average loss at step 4500: 3.227015 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.76\n",
      "Validation set perplexity: 22.07\n",
      "Average loss at step 4600: 3.262052 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.78\n",
      "Validation set perplexity: 21.08\n",
      "Average loss at step 4700: 3.273052 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.43\n",
      "Validation set perplexity: 22.12\n",
      "Average loss at step 4800: 3.264903 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.73\n",
      "Validation set perplexity: 22.12\n",
      "Average loss at step 4900: 3.273338 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.87\n",
      "Validation set perplexity: 21.96\n",
      "Average loss at step 5000: 3.272677 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.31\n",
      "================================================================================\n",
      "ar some siers and as was in networ africa were indmam one nine eight eight one nine one two six deforms in castized bew one eight five nine one zero zero one se\n",
      "an imaligin off ruse in since the ackes new fopmo one to the alow ter manyted any more the had became nabined to stations antilrs was a jame mal had fedence res\n",
      "axy of lyrts a paris west that havd or other one and to ho two sugiasast ojczy to mosu demains and the tyred this in interphetic and terreby gam setiken beld at\n",
      "al eight one eight zero zero writt clau contact potribut racks one neven seven four cranis iverang frammer aircraft serink on the engless more was brandiessian \n",
      "ahary a c wer fember ring dieseric murarially key bash peale jould theirque to rus ust and mean possor of contings a heances ruth singing near page and move pad\n",
      "================================================================================\n",
      "Validation set perplexity: 20.99\n",
      "Average loss at step 5100: 3.198174 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.75\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 5200: 3.208085 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.63\n",
      "Validation set perplexity: 20.37\n",
      "Average loss at step 5300: 3.261496 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.48\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 5400: 3.258739 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.46\n",
      "Validation set perplexity: 20.09\n",
      "Average loss at step 5500: 3.237013 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.34\n",
      "Validation set perplexity: 19.82\n",
      "Average loss at step 5600: 3.189739 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.83\n",
      "Validation set perplexity: 19.80\n",
      "Average loss at step 5700: 3.194532 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.88\n",
      "Validation set perplexity: 19.95\n",
      "Average loss at step 5800: 3.237411 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.66\n",
      "Validation set perplexity: 19.93\n",
      "Average loss at step 5900: 3.218728 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.83\n",
      "Validation set perplexity: 19.88\n",
      "Average loss at step 6000: 3.207697 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.04\n",
      "================================================================================\n",
      "aw valb or not of oratak rusentencted by a nine six three one zero sevge arivary bnstable stronguain tet in the one eight seven msure of his began regionsated c\n",
      "avy ger s game chared the has de intuement saces in the preserenced as such as reage of the broka aim sy abegius carly vybication or dohn politing moverple a di\n",
      "afkan commistance of inforponcons the cavarated to namestry been be jobys seasipm on pon both storard quoting wodding wibea all ex transmite internations bleirh\n",
      "aim properent after codcanies hrince of threations whese utish their of give eight sural falen solfield to seele of devory liber with the grads as from saple ra\n",
      "ar won noxher propperaseion suscoi eoa of follgs amony only woegs capation of gremanume talf of any used in a greern o repentam in one evest six externment i or\n",
      "================================================================================\n",
      "Validation set perplexity: 19.75\n",
      "Average loss at step 6100: 3.211344 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.92\n",
      "Validation set perplexity: 19.95\n",
      "Average loss at step 6200: 3.203881 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.16\n",
      "Validation set perplexity: 19.68\n",
      "Average loss at step 6300: 3.166509 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.19\n",
      "Validation set perplexity: 19.54\n",
      "Average loss at step 6400: 3.207334 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.33\n",
      "Validation set perplexity: 19.52\n",
      "Average loss at step 6500: 3.191268 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.87\n",
      "Validation set perplexity: 19.52\n",
      "Average loss at step 6600: 3.187205 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.96\n",
      "Validation set perplexity: 19.62\n",
      "Average loss at step 6700: 3.182175 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.60\n",
      "Validation set perplexity: 19.52\n",
      "Average loss at step 6800: 3.190729 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.79\n",
      "Validation set perplexity: 19.49\n",
      "Average loss at step 6900: 3.175410 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.35\n",
      "Validation set perplexity: 19.58\n",
      "Average loss at step 7000: 3.189218 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.74\n",
      "================================================================================\n",
      "acantou kle as the himmer generable the muchept uk later charnated are by early bantiel t virpegal pactuls their to the pundarratic united is low states legista\n",
      "ax instrear to contaticud as award wrolotzary for banine frulowing acheivansselling sector two usfal shur arur shiut longer one nine six zero th use deaderea cr\n",
      "aken whise councry invesuld furtex of the canturns infreging would buid cound lastenke with this were chu some they not intending that here internally by humnit\n",
      "ary ingine mina inmorpomined nf these can quing who repolative pett in the uded to strook make of cirist elector ced the number five herwar pwepignant usis comi\n",
      "aje part out say we awucwn magic sirdicing her came leading to wield any pariously word is states suriouts ats pcultating the nopex and dest in the roun these o\n",
      "================================================================================\n",
      "Validation set perplexity: 19.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "              'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample_id(random_distribution())\n",
    "                    sentence = characters([feed])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: [feed]})\n",
    "                        feed = sample_id(prediction)\n",
    "                        sentence += characters([feed])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
